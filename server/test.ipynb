{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_upstage import ChatUpstage\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('../')\n",
    "api_key = os.environ['UPSTAGE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing pdf into html...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsing pdf into html...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m layzer \u001b[38;5;241m=\u001b[39m UpstageLayoutAnalysisLoader(pdf_filepath, output_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mlayzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/site-packages/langchain_upstage/layout_analysis.py:180\u001b[0m, in \u001b[0;36mUpstageLayoutAnalysisLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m blob \u001b[38;5;241m=\u001b[39m Blob\u001b[38;5;241m.\u001b[39mfrom_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n\u001b[1;32m    173\u001b[0m parser \u001b[38;5;241m=\u001b[39m UpstageLayoutAnalysisParser(\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key,\n\u001b[1;32m    175\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m     exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude,\n\u001b[1;32m    179\u001b[0m )\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/site-packages/langchain_upstage/layout_analysis_parsers.py:352\u001b[0m, in \u001b[0;36mUpstageLayoutAnalysisParser.lazy_parse\u001b[0;34m(self, blob, is_batch)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_page \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m number_of_pages:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_and_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_pages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m elements:\n\u001b[1;32m    354\u001b[0m     result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m parse_output(element, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_type)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/site-packages/langchain_upstage/layout_analysis_parsers.py:247\u001b[0m, in \u001b[0;36mUpstageLayoutAnalysisParser._split_and_request\u001b[0;34m(self, full_docs, start_page, num_pages)\u001b[0m\n\u001b[1;32m    245\u001b[0m     merger\u001b[38;5;241m.\u001b[39mwrite(buffer)\n\u001b[1;32m    246\u001b[0m     buffer\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 247\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/site-packages/langchain_upstage/layout_analysis_parsers.py:193\u001b[0m, in \u001b[0;36mUpstageLayoutAnalysisParser._get_response\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    189\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    190\u001b[0m         LAYOUT_ANALYSIS_URL, headers\u001b[38;5;241m=\u001b[39mheaders, files\u001b[38;5;241m=\u001b[39mfiles, data\u001b[38;5;241m=\u001b[39moptions\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLAYOUT_ANALYSIS_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    198\u001b[0m result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/upstage/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load pdf into html file\n",
    "pdf_filepath=\"./sample_pdfs/Attention Is All You Need.pdf\"\n",
    "print('parsing pdf into html...')\n",
    "layzer = UpstageLayoutAnalysisLoader(pdf_filepath, output_type=\"html\")\n",
    "docs = layzer.load()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: 78\n",
      "unique_splits: 77\n"
     ]
    }
   ],
   "source": [
    "# split text into text chunks\n",
    "text_spliiter = RecursiveCharacterTextSplitter.from_language(\n",
    "    chunk_size=1000, chunk_overlap=100, language=Language.HTML\n",
    ")\n",
    "splits = text_spliiter.split_documents(docs)\n",
    "print(f\"Splits: {len(splits)}\")\n",
    "unique_splits=[]\n",
    "for split in splits:\n",
    "    if split not in unique_splits:\n",
    "        unique_splits.append(split)\n",
    "print(f\"unique_splits: {len(unique_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings for the given text chunks and save it\n",
    "embedding_name=\"attention\"\n",
    "persist_directory=f\"./chroma_db/{embedding_name}/\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=unique_splits,\n",
    "    ids=[doc.page_content for doc in unique_splits],\n",
    "    embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    persist_directory=persist_directory,\n",
    ")\n",
    "\n",
    "# vectorstore = Chroma(\n",
    "#     persist_directory=f\"./chroma_db/{embedding_name}\",\n",
    "#     embedding_function=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['<br>',\n",
       "  '<br><h1 id=\\'5\\' style=\\'font-size:16px\\'>Attention Visualizations</h1><figure><img id=\\'6\\' style=\\'font-size:20px\\' alt=\"It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS>',\n",
       "  \"<br><p id='1' data-category='paragraph' style='font-size:18px'>Provided proper attribution is provided, Google hereby grants permission to<br>reproduce the tables and figures in this paper solely for use in journalistic or<br>scholarly works.</p><h1 id='2' style='font-size:20px'>Attention Is All You Need</h1><table id='3' style='font-size:16px'><tr><td>Ashish Vaswani ∗</td><td>Noam Shazeer ∗</td><td>Niki Parmar ∗</td><td>Jakob Uszkoreit ∗</td></tr><tr><td>Google Brain</td><td>Google Brain</td><td>Google Research</td><td>Google Research</td></tr><tr><td>avaswani@google.com</td><td>noam@google.com</td><td>nikip@google.com</td><td>usz@google.com</td></tr></table><table id='4' style='font-size:16px'><tr><td>Llion Jones ∗</td><td>Aidan N. Gomez ∗ †</td><td>Łukasz Kaiser ∗</td></tr><tr><td>Google Research</td><td>University of Toronto</td><td>Google Brain</td></tr><tr><td>llion@google.com</td><td>aidan@cs.toronto.edu</td><td>lukaszkaiser@google.com</td></tr></table>\",\n",
       "  '<br>Language Processing , pages 832–841. ACL, August 2009.<br>[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring<br>the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.<br>[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural<br>Information Processing Systems, (NIPS) , 2016.<br>[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference<br>on Learning Representations (ICLR) , 2016.<br>[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-<br>ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,<br>2017.<br>[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.<br>In International Conference on Learning Representations , 2017.<br>[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.',\n",
       "  '<br>Learning Research , 15(1):1929–1958, 2014.<br>[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory<br>networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,<br>Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,<br>Inc., 2015.<br>[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural<br>networks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.<br>[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.<br>Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.<br>[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In<br>Advances in Neural Information Processing Systems , 2015.<br>[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang',\n",
       "  '<br>[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.<br>[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint<br>arXiv:1703.10722 , 2017.<br>[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen<br>Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint<br>arXiv:1703.03130 , 2017.<br>[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task<br>sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.<br>[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-<br>based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.</p>',\n",
       "  \"<br>[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang<br>Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine<br>translation system: Bridging the gap between human and machine translation. arXiv preprint<br>arXiv:1609.08144 , 2016.<br>[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with<br>fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.<br>[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate<br>shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume<br>1: Long Papers) , pages 434–443. ACL, August 2013.</p><h1 id='4' style='font-size:20px'>Input-Input Layer5</h1>\",\n",
       "  '<br>all positions in the decoder up to and including that position. We need to prevent leftward<br>information flow in the decoder to preserve the auto-regressive property. We implement this<br>inside of scaled dot-product attention by masking out (setting to −∞ ) all values in the input<br>of the softmax which correspond to illegal connections. See Figure 2.</p><br>',\n",
       "  '<br>and interpretable tree annotation. In Proceedings of the 21st International Conference on<br>Computational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July<br>2006.<br>[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv<br>preprint arXiv:1608.05859 , 2016.<br>[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words<br>with subword units. arXiv preprint arXiv:1508.07909 , 2015.<br>[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,<br>and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts<br>layer. arXiv preprint arXiv:1701.06538 , 2017.<br>[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-<br>nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine<br>Learning Research , 15(1):1929–1958, 2014.',\n",
       "  '<br>our model establishes a new single-model state-of-the-art BLEU score of 41.8 after<br>training for 3.5 days on eight GPUs, a small fraction of the training costs of the<br>best models from the literature. We show that the Transformer generalizes well to<br>other tasks by applying it successfully to English constituency parsing both with<br>large and limited training data.</p>',\n",
       "  '<br>per-word perplexities.</p>',\n",
       "  '<br>tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.<br>[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint<br>arXiv:1308.0850 , 2013.<br>[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-<br>age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern<br>Recognition , pages 770–778, 2016.<br>[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in<br>recurrent nets: the difficulty of learning long-term dependencies, 2001.<br>[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,<br>9(8):1735–1780, 1997.<br>[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations<br>across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural<br>Language Processing , pages 832–841. ACL, August 2009.',\n",
       "  \"<p id='0' data-category='paragraph' style='font-size:14px'>[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,<br>and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical<br>machine translation. CoRR , abs/1406.1078, 2014.<br>[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv<br>preprint arXiv:1610.02357 , 2016.<br>[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation<br>of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.<br>[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural<br>network grammars. In Proc. of NAACL , 2016.<br>[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-<br>tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\",\n",
       "  \"<p id='101' data-category='paragraph' style='font-size:16px'>On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)<br>in Table 2) outperforms the best previously reported models (including ensembles) by more than 2 . 0<br>BLEU, establishing a new state-of-the-art BLEU score of 28 . 4 . The configuration of this model is<br>listed in the bottom line of Table 3. Training took 3 . 5 days on 8 P100 GPUs. Even our base model<br>surpasses all previously published models and ensembles, at a fraction of the training cost of any of<br>the competitive models.</p><br>\",\n",
       "  \"<p id='102' data-category='paragraph' style='font-size:14px'>On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41 . 0 ,<br>outperforming all of the previously published single models, at less than 1 / 4 the training cost of the<br>previous state-of-the-art model. The Transformer (big) model trained for English-to-French used<br>dropout rate P drop = 0 . 1 , instead of 0 . 3 .</p><br><p id='103' data-category='paragraph' style='font-size:16px'>For the base models, we used a single model obtained by averaging the last 5 checkpoints, which<br>were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We<br>used beam search with a beam size of 4 and length penalty α = 0 . 6 [ 38 ]. These hyperparameters<br>were chosen after experimentation on the development set. We set the maximum output length during<br>inference to input length + 50 , but terminate early when possible [38].</p><br>\",\n",
       "  \"<p id='104' data-category='paragraph' style='font-size:16px'>Table 2 summarizes our results and compares our translation quality and training costs to other model<br>architectures from the literature. We estimate the number of floating point operations used to train a<br>model by multiplying the training time, the number of GPUs used, and an estimate of the sustained<br>single-precision floating-point capacity of each GPU 5 .</p><h1 id='105' style='font-size:20px'>6.2 Model Variations</h1><br><p id='106' data-category='paragraph' style='font-size:18px'>To evaluate the importance of different components of the Transformer, we varied our base model<br>in different ways, measuring the change in performance on English-to-German translation on the</p><p id='107' data-category='paragraph' style='font-size:14px'>5 We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.</p>\",\n",
       "  \"<p id='109' data-category='paragraph' style='font-size:16px'>Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base<br>model. All metrics are on the English-to-German translation development set, newstest2013. Listed<br>perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\",\n",
       "  \"<p id='111' data-category='paragraph' style='font-size:16px'>development set, newstest2013. We used beam search as described in the previous section, but no<br>checkpoint averaging. We present these results in Table 3.</p><br><p id='112' data-category='paragraph' style='font-size:16px'>In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,<br>keeping the amount of computation constant, as described in Section 3.2.2. While single-head<br>attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.</p><br>\",\n",
       "  \"<p id='113' data-category='paragraph' style='font-size:14px'>In Table 3 rows (B), we observe that reducing the attention key size d k hurts model quality. This<br>suggests that determining compatibility is not easy and that a more sophisticated compatibility<br>function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,<br>bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our<br>sinusoidal positional encoding with learned positional embeddings [ 9 ], and observe nearly identical<br>results to the base model.</p><h1 id='114' style='font-size:20px'>6.3 English Constituency Parsing</h1><br>\",\n",
       "  \"<p id='115' data-category='paragraph' style='font-size:16px'>To evaluate if the Transformer can generalize to other tasks we performed experiments on English<br>constituency parsing. This task presents specific challenges: the output is subject to strong structural<br>constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence<br>models have not been able to attain state-of-the-art results in small-data regimes [37].</p><br><p id='116' data-category='paragraph' style='font-size:14px'>We trained a 4-layer transformer with d model = 1024 on the Wall Street Journal (WSJ) portion of the<br>Penn Treebank [ 25 ], about 40K training sentences. We also trained it in a semi-supervised setting,<br>using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences<br>[ 37 ]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens<br>for the semi-supervised setting.</p><br>\",\n",
       "  \"<p id='117' data-category='paragraph' style='font-size:16px'>We performed only a small number of experiments to select the dropout, both attention and residual<br>(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters<br>remained unchanged from the English-to-German base translation model. During inference, we</p>\",\n",
       "  \"<p id='119' data-category='paragraph' style='font-size:18px'>Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23<br>of WSJ)</p>\",\n",
       "  \"<p id='121' data-category='paragraph' style='font-size:14px'>increased the maximum output length to input length + 300 . We used a beam size of 21 and α = 0 . 3<br>for both WSJ only and the semi-supervised setting.</p><br><p id='122' data-category='paragraph' style='font-size:18px'>Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-<br>prisingly well, yielding better results than all previously reported models with the exception of the<br>Recurrent Neural Network Grammar [8].</p><br><p id='123' data-category='paragraph' style='font-size:18px'>In contrast to RNN sequence-to-sequence models [ 37 ], the Transformer outperforms the Berkeley-<br>Parser [29] even when training only on the WSJ training set of 40K sentences.</p><br><h1 id='124' style='font-size:20px'>7 Conclusion</h1><br>\",\n",
       "  \"<p id='125' data-category='paragraph' style='font-size:18px'>In this work, we presented the Transformer, the first sequence transduction model based entirely on<br>attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with<br>multi-headed self-attention.</p><br><p id='126' data-category='paragraph' style='font-size:18px'>For translation tasks, the Transformer can be trained significantly faster than architectures based<br>on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014<br>English-to-French translation tasks, we achieve a new state of the art. In the former task our best<br>model outperforms even all previously reported ensembles.</p><br>\",\n",
       "  \"<p id='127' data-category='paragraph' style='font-size:18px'>We are excited about the future of attention-based models and plan to apply them to other tasks. We<br>plan to extend the Transformer to problems involving input and output modalities other than text and<br>to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs<br>such as images, audio and video. Making generation less sequential is another research goals of ours.</p><br><p id='128' data-category='paragraph' style='font-size:16px'>The code we used to train and evaluate our models is available at https://github.com/<br>tensorflow/tensor2tensor .</p><br><p id='129' data-category='paragraph' style='font-size:18px'>Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful<br>comments, corrections and inspiration.</p><br><p id='130' data-category='paragraph' style='font-size:20px'>References</p><br>\",\n",
       "  \"<p id='13' data-category='paragraph' style='font-size:16px'>Recurrent models typically factor computation along the symbol positions of the input and output<br>sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden<br>states h t , as a function of the previous hidden state h t − 1 and the input for position t . This inherently<br>sequential nature precludes parallelization within training examples, which becomes critical at longer<br>sequence lengths, as memory constraints limit batching across examples. Recent work has achieved<br>significant improvements in computational efficiency through factorization tricks [ 21 ] and conditional<br>computation [ 32 ], while also improving model performance in case of the latter. The fundamental<br>constraint of sequential computation, however, remains.</p><br>\",\n",
       "  \"<p id='130' data-category='paragraph' style='font-size:20px'>References</p><br><p id='131' data-category='paragraph' style='font-size:16px'>[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint<br>arXiv:1607.06450 , 2016.<br>[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly<br>learning to align and translate. CoRR , abs/1409.0473, 2014.<br>[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural<br>machine translation architectures. CoRR , abs/1703.03906, 2017.<br>[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine<br>reading. arXiv preprint arXiv:1601.06733 , 2016.</p>\",\n",
       "  '<p id=\\'14\\' data-category=\\'paragraph\\' style=\\'font-size:16px\\'>The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad></p><p id=\\'15\\' data-category=\\'paragraph\\' style=\\'font-size:18px\\'>The Law will never be perfect Layer5 should be just - this is what we are missing , in my opinion . <EOS> <pad><br>, but its application<br>Input-Input</p><figure><img id=\\'16\\' style=\\'font-size:16px\\' alt=\"The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>\\nThe Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>\" data-coord=\"top-left:(233,830); bottom-right:(1052,1227)\" /></figure>',\n",
       "  \"<p id='14' data-category='paragraph' style='font-size:18px'>Attention mechanisms have become an integral part of compelling sequence modeling and transduc-<br>tion models in various tasks, allowing modeling of dependencies without regard to their distance in<br>the input or output sequences [ 2 , 19 ]. In all but a few cases [ 27 ], however, such attention mechanisms<br>are used in conjunction with a recurrent network.</p><br><p id='15' data-category='paragraph' style='font-size:18px'>In this work we propose the Transformer, a model architecture eschewing recurrence and instead<br>relying entirely on an attention mechanism to draw global dependencies between input and output.<br>The Transformer allows for significantly more parallelization and can reach a new state of the art in<br>translation quality after being trained for as little as twelve hours on eight P100 GPUs.</p><h1 id='16' style='font-size:20px'>2 Background</h1>\",\n",
       "  \"<p id='17' data-category='paragraph' style='font-size:18px'>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU<br>[ 16 ], ByteNet [ 18 ] and ConvS2S [ 9 ], all of which use convolutional neural networks as basic building<br>block, computing hidden representations in parallel for all input and output positions. In these models,<br>the number of operations required to relate signals from two arbitrary input or output positions grows<br>in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes<br>it more difficult to learn dependencies between distant positions [ 12 ]. In the Transformer this is<br>reduced to a constant number of operations, albeit at the cost of reduced effective resolution due<br>to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as<br>described in section 3.2.</p><br>\",\n",
       "  \"<p id='18' data-category='paragraph' style='font-size:18px'>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions<br>of a single sequence in order to compute a representation of the sequence. Self-attention has been<br>used successfully in a variety of tasks including reading comprehension, abstractive summarization,<br>textual entailment and learning task-independent sentence representations [4, 27, 28, 22].</p><br><p id='19' data-category='paragraph' style='font-size:18px'>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-<br>aligned recurrence and have been shown to perform well on simple-language question answering and<br>language modeling tasks [34].</p><br>\",\n",
       "  \"<p id='2' data-category='paragraph' style='font-size:14px'>[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated<br>corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.<br>[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In<br>Proceedings of the Human Language Technology Conference of the NAACL, Main Conference ,<br>pages 152–159. ACL, June 2006.<br>[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention<br>model. In Empirical Methods in Natural Language Processing , 2016.<br>[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive<br>summarization. arXiv preprint arXiv:1705.04304 , 2017.<br>[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,<br>and interpretable tree annotation. In Proceedings of the 21st International Conference on\",\n",
       "  \"<p id='20' data-category='paragraph' style='font-size:18px'>To the best of our knowledge, however, the Transformer is the first transduction model relying<br>entirely on self-attention to compute representations of its input and output without using sequence-<br>aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate<br>self-attention and discuss its advantages over models such as [17, 18] and [9].</p><h1 id='21' style='font-size:20px'>3 Model Architecture</h1>\",\n",
       "  '<p id=\\'22\\' data-category=\\'paragraph\\' style=\\'font-size:14px\\'>Most competitive neural sequence transduction models have an encoder-decoder structure [ 5 , 2 , 35 ].<br>Here, the encoder maps an input sequence of symbol representations ( x 1 , ..., x n ) to a sequence<br>of continuous representations z = ( z 1 , ..., z n ) . Given z , the decoder then generates an output<br>sequence ( y 1 , ..., y m ) of symbols one element at a time. At each step the model is auto-regressive<br>[10], consuming the previously generated symbols as additional input when generating the next.</p><figure><img id=\\'24\\' alt=\"\" data-coord=\"top-left:(402,143); bottom-right:(872,833)\" /></figure><br><caption id=\\'25\\' style=\\'font-size:16px\\'>Figure 1: The Transformer - model architecture.</caption>',\n",
       "  \"<p id='26' data-category='paragraph' style='font-size:16px'>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully<br>connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,<br>respectively.</p><br><h1 id='27' style='font-size:20px'>3.1 Encoder and Decoder Stacks</h1>\",\n",
       "  \"<p id='28' data-category='paragraph' style='font-size:14px'>Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two<br>sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-<br>wise fully connected feed-forward network. We employ a residual connection [ 11 ] around each of<br>the two sub-layers, followed by layer normalization [ 1 ]. That is, the output of each sub-layer is<br>LayerNorm( x + Sublayer( x )) , where Sublayer( x ) is the function implemented by the sub-layer<br>itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding<br>layers, produce outputs of dimension d model = 512 .</p>\",\n",
       "  \"<p id='29' data-category='paragraph' style='font-size:14px'>Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two<br>sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head<br>attention over the output of the encoder stack. Similar to the encoder, we employ residual connections<br>around each of the sub-layers, followed by layer normalization. We also modify the self-attention<br>sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This<br>masking, combined with fact that the output embeddings are offset by one position, ensures that the<br>predictions for position i can depend only on the known outputs at positions less than i .</p><br><h1 id='30' style='font-size:20px'>3.2 Attention</h1><br>\",\n",
       "  '<p id=\\'31\\' data-category=\\'paragraph\\' style=\\'font-size:16px\\'>An attention function can be described as mapping a query and a set of key-value pairs to an output,<br>where the query, keys, values, and output are all vectors. The output is computed as a weighted sum</p><h1 id=\\'33\\' style=\\'font-size:20px\\'>Scaled Dot-Product Attention</h1><figure><img id=\\'34\\' alt=\"\" data-coord=\"top-left:(351,196); bottom-right:(505,465)\" /></figure><br><h1 id=\\'35\\' style=\\'font-size:20px\\'>Multi-Head Attention</h1><figure><img id=\\'36\\' alt=\"\" data-coord=\"top-left:(715,168); bottom-right:(975,505)\" /></figure><p id=\\'37\\' data-category=\\'paragraph\\' style=\\'font-size:20px\\'>Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several<br>attention layers running in parallel.</p>',\n",
       "  '<p id=\\'38\\' data-category=\\'paragraph\\' style=\\'font-size:20px\\'>of the values, where the weight assigned to each value is computed by a compatibility function of the<br>query with the corresponding key.</p><br><h1 id=\\'39\\' style=\\'font-size:22px\\'>3.2.1 Scaled Dot-Product Attention</h1><p id=\\'40\\' data-category=\\'paragraph\\' style=\\'font-size:18px\\'>We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of<br>queries and keys of dimension d k , and values of dimension d v . We compute the dot products of the<br>query with all keys, divide each by √ d k , and apply a softmax function to obtain the weights on the<br>values.</p><br><p id=\\'41\\' data-category=\\'paragraph\\' style=\\'font-size:18px\\'>In practice, we compute the attention function on a set of queries simultaneously, packed together<br>into a matrix Q . The keys and values are also packed together into matrices K and V . We compute<br>the matrix of outputs as:</p>',\n",
       "  \"<p id='42' data-category='equation' style='font-size:14px'>QK T<br>Attention( Q, K, V ) = softmax( √ d k<br>) V (1)</p><p id='43' data-category='paragraph' style='font-size:18px'>The two most commonly used attention functions are additive attention [ 2 ], and dot-product (multi-<br>plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor<br>of √ d k<br>1 . Additive attention computes the compatibility function using a feed-forward network with<br>a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is<br>much faster and more space-efficient in practice, since it can be implemented using highly optimized<br>matrix multiplication code.</p>\",\n",
       "  \"<p id='44' data-category='paragraph' style='font-size:16px'>While for small values of d k the two mechanisms perform similarly, additive attention outperforms<br>dot product attention without scaling for larger values of d k [ 3 ]. We suspect that for large values of<br>d k , the dot products grow large in magnitude, pushing the softmax function into regions where it has<br>extremely small gradients 4 . To counteract this effect, we scale the dot products by √ d k<br>1 .</p><br><h1 id='45' style='font-size:22px'>3.2.2 Multi-Head Attention</h1>\",\n",
       "  \"<p id='46' data-category='paragraph' style='font-size:16px'>Instead of performing a single attention function with d model -dimensional keys, values and queries,<br>we found it beneficial to linearly project the queries, keys and values h times with different, learned<br>linear projections to d k , d k and d v dimensions, respectively. On each of these projected versions of<br>queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional</p><p id='47' data-category='paragraph' style='font-size:14px'>4 To illustrate why the dot products get large, assume that the components of q and k are independent random<br>d k q i k i , has mean 0 and variance d k .<br>variables with mean 0 and variance 1 . Then their dot product, q · k = � i =1</p><p id='49' data-category='paragraph' style='font-size:20px'>output values. These are concatenated and once again projected, resulting in the final values, as<br>depicted in Figure 2.</p><br>\",\n",
       "  \"<p id='5' data-category='paragraph' style='font-size:16px'>Illia Polosukhin ∗ ‡<br>illia.polosukhin@gmail.com</p><h1 id='6' style='font-size:18px'>Abstract</h1>\",\n",
       "  \"<p id='50' data-category='paragraph' style='font-size:20px'>Multi-head attention allows the model to jointly attend to information from different representation<br>subspaces at different positions. With a single attention head, averaging inhibits this.</p><p id='51' data-category='equation' style='font-size:14px'>MultiHead( Q, K, V ) = Concat(head 1 , ..., head h ) W O<br>Q K , V W i )<br>V<br>where head i = Attention( QW i , KW i</p><p id='52' data-category='paragraph' style='font-size:14px'>Q R d model × d k , W i ∈ R d model × d k , W i ∈ R d model × d v<br>Where the projections are parameter matrices W i ∈<br>K<br>V<br>and W O ∈ R hd v × d model .</p><br>\",\n",
       "  \"<p id='53' data-category='paragraph' style='font-size:16px'>In this work we employ h = 8 parallel attention layers, or heads. For each of these we use<br>d k = d v = d model /h = 64 . Due to the reduced dimension of each head, the total computational cost<br>is similar to that of single-head attention with full dimensionality.</p><br><h1 id='54' style='font-size:22px'>3.2.3 Applications of Attention in our Model</h1><br><h1 id='55' style='font-size:20px'>The Transformer uses multi-head attention in three different ways:</h1><br>\",\n",
       "  '<p id=\\'56\\' data-category=\\'paragraph\\' style=\\'font-size:20px\\'>• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,<br>and the memory keys and values come from the output of the encoder. This allows every<br>position in the decoder to attend over all positions in the input sequence. This mimics the<br>typical encoder-decoder attention mechanisms in sequence-to-sequence models such as<br>[38, 2, 9].<br>• The encoder contains self-attention layers. In a self-attention layer all of the keys, values<br>and queries come from the same place, in this case, the output of the previous layer in the<br>encoder. Each position in the encoder can attend to all positions in the previous layer of the<br>encoder.<br>• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to<br>all positions in the decoder up to and including that position. We need to prevent leftward',\n",
       "  \"<p id='57' data-category='paragraph' style='font-size:22px'>3.3 Position-wise Feed-Forward Networks</p><br><p id='58' data-category='paragraph' style='font-size:20px'>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully<br>connected feed-forward network, which is applied to each position separately and identically. This<br>consists of two linear transformations with a ReLU activation in between.</p><p id='59' data-category='equation' style='font-size:14px'>FFN( x ) = max(0 , xW 1 + b 1 ) W 2 + b 2</p><br><caption id='60' style='font-size:20px'>(2)</caption>\",\n",
       "  \"<p id='61' data-category='paragraph' style='font-size:18px'>While the linear transformations are the same across different positions, they use different parameters<br>from layer to layer. Another way of describing this is as two convolutions with kernel size 1.<br>The dimensionality of input and output is d model = 512 , and the inner-layer has dimensionality<br>d ff = 2048 .</p><h1 id='62' style='font-size:22px'>3.4 Embeddings and Softmax</h1>\",\n",
       "  \"<p id='63' data-category='paragraph' style='font-size:18px'>Similarly to other sequence transduction models, we use learned embeddings to convert the input<br>tokens and output tokens to vectors of dimension d model . We also use the usual learned linear transfor-<br>mation and softmax function to convert the decoder output to predicted next-token probabilities. In<br>our model, we share the same weight matrix between the two embedding layers and the pre-softmax<br>linear transformation, similar to [ 30 ]. In the embedding layers, we multiply those weights by √ d model .</p>\",\n",
       "  \"<p id='65' data-category='paragraph' style='font-size:18px'>Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations<br>for different layer types. n is the sequence length, d is the representation dimension, k is the kernel<br>size of convolutions and r the size of the neighborhood in restricted self-attention.</p><table id='66' style='font-size:14px'><tr><td>Layer Type</td><td>Complexity per Layer</td><td>Sequential Operations</td><td>Maximum Path Length</td></tr><tr><td>Self-Attention</td><td>O ( n 2 · d )</td><td>O (1)</td><td>O (1)</td></tr><tr><td>Recurrent</td><td>O ( n · d 2 )</td><td>O ( n )</td><td>O ( n )</td></tr><tr><td>Convolutional</td><td>O ( k · n · d 2 )</td><td>O (1)</td><td>O ( log k ( n ))</td></tr><tr><td>Self-Attention (restricted)</td><td>O ( r · n · d )</td><td>O (1)</td><td>O ( n/r )</td></tr></table><h1 id='67' style='font-size:20px'>3.5 Positional Encoding</h1>\",\n",
       "  '<p id=\\'68\\' data-category=\\'paragraph\\' style=\\'font-size:18px\\'>Since our model contains no recurrence and no convolution, in order for the model to make use of the<br>order of the sequence, we must inject some information about the relative or absolute position of the<br>tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the<br>bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model<br>as the embeddings, so that the two can be summed. There are many choices of positional encodings,<br>learned and fixed [9].</p><br><p id=\\'69\\' data-category=\\'paragraph\\' style=\\'font-size:18px\\'>In this work, we use sine and cosine functions of different frequencies:</p><p id=\\'70\\' data-category=\\'equation\\' style=\\'font-size:14px\\'>PE ( pos, 2 i ) = sin ( pos/ 10000 2 i/d model )<br>PE ( pos, 2 i +1) = cos ( pos/ 10000 2 i/d model )</p>',\n",
       "  \"<p id='7' data-category='paragraph' style='font-size:16px'>The dominant sequence transduction models are based on complex recurrent or<br>convolutional neural networks that include an encoder and a decoder. The best<br>performing models also connect the encoder and decoder through an attention<br>mechanism. We propose a new simple network architecture, the Transformer,<br>based solely on attention mechanisms, dispensing with recurrence and convolutions<br>entirely. Experiments on two machine translation tasks show these models to<br>be superior in quality while being more parallelizable and requiring significantly<br>less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-<br>to-German translation task, improving over the existing best results, including<br>ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,<br>our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\",\n",
       "  \"<p id='71' data-category='paragraph' style='font-size:16px'>where pos is the position and i is the dimension. That is, each dimension of the positional encoding<br>corresponds to a sinusoid. The wavelengths form a geometric progression from 2 π to 10000 · 2 π . We<br>chose this function because we hypothesized it would allow the model to easily learn to attend by<br>relative positions, since for any fixed offset k , PE pos + k can be represented as a linear function of<br>PE pos .</p><br><p id='72' data-category='paragraph' style='font-size:18px'>We also experimented with using learned positional embeddings [ 9 ] instead, and found that the two<br>versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version<br>because it may allow the model to extrapolate to sequence lengths longer than the ones encountered<br>during training.</p><br><h1 id='73' style='font-size:22px'>4 Why Self-Attention</h1>\",\n",
       "  \"<p id='74' data-category='paragraph' style='font-size:16px'>In this section we compare various aspects of self-attention layers to the recurrent and convolu-<br>tional layers commonly used for mapping one variable-length sequence of symbol representations<br>( x 1 , ..., x n ) to another sequence of equal length ( z 1 , ..., z n ) , with x i , z i ∈ R d , such as a hidden<br>layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we<br>consider three desiderata.</p><br><p id='75' data-category='paragraph' style='font-size:18px'>One is the total computational complexity per layer. Another is the amount of computation that can<br>be parallelized, as measured by the minimum number of sequential operations required.</p><br>\",\n",
       "  \"<p id='76' data-category='paragraph' style='font-size:18px'>The third is the path length between long-range dependencies in the network. Learning long-range<br>dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the<br>ability to learn such dependencies is the length of the paths forward and backward signals have to<br>traverse in the network. The shorter these paths between any combination of positions in the input<br>and output sequences, the easier it is to learn long-range dependencies [ 12 ]. Hence we also compare<br>the maximum path length between any two input and output positions in networks composed of the<br>different layer types.</p><br>\",\n",
       "  \"<p id='77' data-category='paragraph' style='font-size:18px'>As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially<br>executed operations, whereas a recurrent layer requires O ( n ) sequential operations. In terms of<br>computational complexity, self-attention layers are faster than recurrent layers when the sequence</p>\",\n",
       "  \"<p id='79' data-category='paragraph' style='font-size:16px'>length n is smaller than the representation dimensionality d , which is most often the case with<br>sentence representations used by state-of-the-art models in machine translations, such as word-piece<br>[ 38 ] and byte-pair [ 31 ] representations. To improve computational performance for tasks involving<br>very long sequences, self-attention could be restricted to considering only a neighborhood of size r in<br>the input sequence centered around the respective output position. This would increase the maximum<br>path length to O ( n/r ) . We plan to investigate this approach further in future work.</p><br>\",\n",
       "  \"<p id='8' data-category='paragraph' style='font-size:14px'>∗ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started<br>the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and<br>has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head<br>attention and the parameter-free position representation and became the other person involved in nearly every<br>detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and<br>tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and<br>efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and<br>implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating<br>our research.</p><br>\",\n",
       "  \"<p id='80' data-category='paragraph' style='font-size:16px'>A single convolutional layer with kernel width k < n does not connect all pairs of input and output<br>positions. Doing so requires a stack of O ( n/k ) convolutional layers in the case of contiguous kernels,<br>or O ( log k ( n )) in the case of dilated convolutions [ 18 ], increasing the length of the longest paths<br>between any two positions in the network. Convolutional layers are generally more expensive than<br>recurrent layers, by a factor of k . Separable convolutions [ 6 ], however, decrease the complexity<br>considerably, to O ( k · n · d + n · d 2 ) . Even with k = n , however, the complexity of a separable<br>convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,<br>the approach we take in our model.</p><br>\",\n",
       "  \"<p id='81' data-category='paragraph' style='font-size:18px'>As side benefit, self-attention could yield more interpretable models. We inspect attention distributions<br>from our models and present and discuss examples in the appendix. Not only do individual attention<br>heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic<br>and semantic structure of the sentences.</p><h1 id='82' style='font-size:22px'>5 Training</h1><h1 id='83' style='font-size:18px'>This section describes the training regime for our models.</h1><h1 id='84' style='font-size:20px'>5.1 Training Data and Batching</h1><br>\",\n",
       "  \"<p id='85' data-category='paragraph' style='font-size:18px'>We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million<br>sentence pairs. Sentences were encoded using byte-pair encoding [ 3 ], which has a shared source-<br>target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT<br>2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece<br>vocabulary [ 38 ]. Sentence pairs were batched together by approximate sequence length. Each training<br>batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000<br>target tokens.</p><h1 id='86' style='font-size:20px'>5.2 Hardware and Schedule</h1><br>\",\n",
       "  \"<p id='87' data-category='paragraph' style='font-size:18px'>We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using<br>the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We<br>trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the<br>bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps<br>(3.5 days).</p><h1 id='88' style='font-size:20px'>5.3 Optimizer</h1><p id='89' data-category='paragraph' style='font-size:14px'>We used the Adam optimizer [ 20 ] with β 1 = 0 . 9 , β 2 = 0 . 98 and ϵ = 10 − 9 . We varied the learning<br>rate over the course of training, according to the formula:</p><p id='90' data-category='equation' style='font-size:14px'>lrate = d − 0 . 5 · min( step _ num − 0 . 5 , step _ num · warmup _ steps − 1 . 5 ) (3)<br>model</p>\",\n",
       "  \"<p id='9' data-category='paragraph' style='font-size:14px'>† Work performed while at Google Brain.<br>‡ Work performed while at Google Research.</p><p id='10' data-category='paragraph' style='font-size:14px'>31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.</p><h1 id='11' style='font-size:20px'>1 Introduction</h1><p id='12' data-category='paragraph' style='font-size:18px'>Recurrent neural networks, long short-term memory [ 13 ] and gated recurrent [ 7 ] neural networks<br>in particular, have been firmly established as state of the art approaches in sequence modeling and<br>transduction problems such as language modeling and machine translation [ 35 , 2 , 5 ]. Numerous<br>efforts have since continued to push the boundaries of recurrent language models and encoder-decoder<br>architectures [38, 24, 15].</p><br>\",\n",
       "  \"<p id='91' data-category='paragraph' style='font-size:16px'>This corresponds to increasing the learning rate linearly for the first warmup _ steps training steps,<br>and decreasing it thereafter proportionally to the inverse square root of the step number. We used<br>warmup _ steps = 4000 .</p><h1 id='92' style='font-size:20px'>5.4 Regularization</h1><br><p id='93' data-category='paragraph' style='font-size:18px'>We employ three types of regularization during training:</p>\",\n",
       "  \"<p id='95' data-category='paragraph' style='font-size:18px'>Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the<br>English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.</p>\",\n",
       "  \"<p id='97' data-category='paragraph' style='font-size:16px'>Residual Dropout We apply dropout [ 33 ] to the output of each sub-layer, before it is added to the<br>sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the<br>positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of<br>P drop = 0 . 1 .</p><p id='98' data-category='paragraph' style='font-size:16px'>Label Smoothing During training, we employed label smoothing of value ϵ ls = 0 . 1 [ 36 ]. This<br>hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.</p><h1 id='99' style='font-size:22px'>6 Results</h1><h1 id='100' style='font-size:20px'>6.1 Machine Translation</h1>\",\n",
       "  '<pad>\\nThe Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>\\nInput-Input Layer5\\nThe Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>\\nThe Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>\" data-coord=\"top-left:(235,351); bottom-right:(1063,1269)\" /></figure><br><p id=\\'11\\' data-category=\\'paragraph\\' style=\\'font-size:14px\\'>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:<br>Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5<br>and 6. Note that the attentions are very sharp for this word.</p><h1 id=\\'13\\' style=\\'font-size:20px\\'>Input-Input Layer5</h1>',\n",
       "  '<pad> <pad> <pad> <pad> <pad> <pad>\\nIt is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad>\" data-coord=\"top-left:(235,209); bottom-right:(1062,638)\" /></figure><br><caption id=\\'7\\' style=\\'font-size:14px\\'>Figure 3: An example of the attention mechanism following long-distance dependencies in the<br>encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of<br>the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for<br>the word ‘making’. Different colors represent different heads. Best viewed in color.</caption><h1 id=\\'9\\' style=\\'font-size:20px\\'>Input-Input Layer5</h1><figure><img id=\\'10\\' style=\\'font-size:16px\\' alt=\"The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS>',\n",
       "  '<pad>\" data-coord=\"top-left:(233,830); bottom-right:(1052,1227)\" /></figure><p id=\\'17\\' data-category=\\'paragraph\\' style=\\'font-size:14px\\'>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the<br>sentence. We give two such examples above, from two different heads from the encoder self-attention<br>at layer 5 of 6. The heads clearly learned to perform different tasks.</p>',\n",
       "  '<table id=\\'110\\' style=\\'font-size:14px\\'><tr><td></td><td>N</td><td>d model</td><td>d ff</td><td>h</td><td>d k</td><td>d v</td><td>P drop</td><td>ϵ ls</td><td>train steps</td><td>PPL (dev)</td><td>BLEU (dev)</td><td>params × 10 6</td></tr><tr><td>base</td><td>6</td><td>512</td><td>2048</td><td>8</td><td>64</td><td>64</td><td>0.1</td><td>0.1</td><td>100K</td><td>4.92</td><td>25.8</td><td>65</td></tr><tr><td rowspan=\"4\">(A)</td><td></td><td></td><td></td><td>1</td><td>512</td><td>512</td><td></td><td></td><td></td><td>5.29</td><td>24.9</td><td></td></tr><tr><td></td><td></td><td></td><td>4</td><td>128</td><td>128</td><td></td><td></td><td></td><td>5.00</td><td>25.5</td><td></td></tr><tr><td></td><td></td><td></td><td>16</td><td>32</td><td>32</td><td></td><td></td><td></td><td>4.91</td><td>25.8</td><td></td></tr><tr><td></td><td></td><td></td><td>32</td><td>16</td><td>16</td><td></td><td></td><td></td><td>5.01</td><td>25.4</td><td></td></tr>',\n",
       "  \"<table id='120' style='font-size:18px'><tr><td>Parser</td><td>Training</td><td>WSJ 23 F1</td></tr><tr><td>Vinyals & Kaiser el al. (2014) [37]</td><td>WSJ only, discriminative</td><td>88.3</td></tr><tr><td>Petrov et al. (2006) [29]</td><td>WSJ only, discriminative</td><td>90.4</td></tr><tr><td>Zhu et al. (2013) [40]</td><td>WSJ only, discriminative</td><td>90.4</td></tr><tr><td>Dyer et al. (2016) [8]</td><td>WSJ only, discriminative</td><td>91.7</td></tr><tr><td>Transformer (4 layers)</td><td>WSJ only, discriminative</td><td>91.3</td></tr><tr><td>Zhu et al. (2013) [40]</td><td>semi-supervised</td><td>91.3</td></tr><tr><td>Huang & Harper (2009) [14]</td><td>semi-supervised</td><td>91.3</td></tr><tr><td>McClosky et al. (2006) [26]</td><td>semi-supervised</td><td>92.1</td></tr><tr><td>Vinyals & Kaiser el al. (2014) [37]</td><td>semi-supervised</td><td>92.1</td></tr><tr><td>Transformer (4 layers)</td><td>semi-supervised</td><td>92.7</td></tr>\",\n",
       "  '<table id=\\'96\\' style=\\'font-size:14px\\'><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">BLEU</td><td colspan=\"2\">Training Cost (FLOPs)</td></tr><tr><td>EN-DE</td><td>EN-FR</td><td>EN-DE</td><td>EN-FR</td></tr><tr><td>ByteNet [18]</td><td>23.75</td><td></td><td></td><td></td></tr><tr><td>Deep-Att + PosUnk [39]</td><td></td><td>39.2</td><td></td><td>1 . 0 · 10 20</td></tr><tr><td>GNMT + RL [38]</td><td>24.6</td><td>39.92</td><td>2 . 3 · 10 19</td><td>1 . 4 · 10 20</td></tr><tr><td>ConvS2S [9]</td><td>25.16</td><td>40.46</td><td>9 . 6 · 10 18</td><td>1 . 5 · 10 20</td></tr><tr><td>MoE [32]</td><td>26.03</td><td>40.56</td><td>2 . 0 · 10 19</td><td>1 . 2 · 10 20</td></tr><tr><td>Deep-Att + PosUnk Ensemble [39]</td><td></td><td>40.4</td><td></td><td>8 . 0 · 10 20</td></tr><tr><td>GNMT + RL Ensemble [38]</td><td>26.30</td><td>41.16</td><td>1 . 8 · 10 20</td><td>1 . 1 · 10 21</td></tr><tr><td>ConvS2S Ensemble [9]</td><td>26.36</td><td>41.29</td><td>7 . 7 · 10 19</td><td>1 . 2 · 10 21</td></tr>',\n",
       "  '<tr><td rowspan=\"2\">(B)</td><td></td><td></td><td></td><td></td><td>16</td><td></td><td></td><td></td><td></td><td>5.16</td><td>25.1</td><td>58</td></tr><tr><td></td><td></td><td></td><td></td><td>32</td><td></td><td></td><td></td><td></td><td>5.01</td><td>25.4</td><td>60</td></tr><tr><td rowspan=\"7\">(C)</td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>6.11</td><td>23.7</td><td>36</td></tr><tr><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.19</td><td>25.3</td><td>50</td></tr><tr><td>8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.88</td><td>25.5</td><td>80</td></tr><tr><td></td><td>256</td><td></td><td></td><td>32</td><td>32</td><td></td><td></td><td></td><td>5.75</td><td>24.5</td><td>28</td></tr><tr><td></td><td>1024</td><td></td><td></td><td>128</td><td>128</td><td></td><td></td><td></td><td>4.66</td><td>26.0</td><td>168</td></tr>',\n",
       "  '<tr><td></td><td></td><td>1024</td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.12</td><td>25.4</td><td>53</td></tr><tr><td></td><td></td><td>4096</td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.75</td><td>26.2</td><td>90</td></tr><tr><td rowspan=\"4\">(D)</td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td></td><td>5.77</td><td>24.6</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td></td><td>4.95</td><td>25.5</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td>4.67</td><td>25.3</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td>5.47</td><td>25.7</td><td></td></tr><tr><td>(E)</td><td colspan=\"9\">positional embedding instead of sinusoids</td><td>4.92</td><td>25.7</td><td></td></tr>',\n",
       "  '<tr><td>Transformer (4 layers)</td><td>semi-supervised</td><td>92.7</td></tr><tr><td>Luong et al. (2015) [23]</td><td>multi-task</td><td>93.0</td></tr><tr><td>Dyer et al. (2016) [8]</td><td>generative</td><td>93.3</td></tr></table>',\n",
       "  '<tr><td>Transformer (base model)</td><td>27.3</td><td>38.1</td><td colspan=\"2\">3 . 3 · 10 18</td></tr><tr><td>Transformer (big)</td><td>28.4</td><td>41.8</td><td colspan=\"2\">2 . 3 · 10 19</td></tr></table>',\n",
       "  '<tr><td>big</td><td>6</td><td>1024</td><td>4096</td><td>16</td><td></td><td></td><td>0.3</td><td></td><td>300K</td><td>4.33</td><td>26.4</td><td>213</td></tr></table>'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15},\n",
       "  {'total_pages': 15}],\n",
       " 'documents': ['<br>',\n",
       "  '<br><h1 id=\\'5\\' style=\\'font-size:16px\\'>Attention Visualizations</h1><figure><img id=\\'6\\' style=\\'font-size:20px\\' alt=\"It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS>',\n",
       "  \"<br><p id='1' data-category='paragraph' style='font-size:18px'>Provided proper attribution is provided, Google hereby grants permission to<br>reproduce the tables and figures in this paper solely for use in journalistic or<br>scholarly works.</p><h1 id='2' style='font-size:20px'>Attention Is All You Need</h1><table id='3' style='font-size:16px'><tr><td>Ashish Vaswani ∗</td><td>Noam Shazeer ∗</td><td>Niki Parmar ∗</td><td>Jakob Uszkoreit ∗</td></tr><tr><td>Google Brain</td><td>Google Brain</td><td>Google Research</td><td>Google Research</td></tr><tr><td>avaswani@google.com</td><td>noam@google.com</td><td>nikip@google.com</td><td>usz@google.com</td></tr></table><table id='4' style='font-size:16px'><tr><td>Llion Jones ∗</td><td>Aidan N. Gomez ∗ †</td><td>Łukasz Kaiser ∗</td></tr><tr><td>Google Research</td><td>University of Toronto</td><td>Google Brain</td></tr><tr><td>llion@google.com</td><td>aidan@cs.toronto.edu</td><td>lukaszkaiser@google.com</td></tr></table>\",\n",
       "  '<br>Language Processing , pages 832–841. ACL, August 2009.<br>[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring<br>the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.<br>[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural<br>Information Processing Systems, (NIPS) , 2016.<br>[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference<br>on Learning Representations (ICLR) , 2016.<br>[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-<br>ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,<br>2017.<br>[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.<br>In International Conference on Learning Representations , 2017.<br>[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.',\n",
       "  '<br>Learning Research , 15(1):1929–1958, 2014.<br>[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory<br>networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,<br>Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,<br>Inc., 2015.<br>[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural<br>networks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.<br>[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.<br>Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.<br>[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In<br>Advances in Neural Information Processing Systems , 2015.<br>[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang',\n",
       "  '<br>[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.<br>[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint<br>arXiv:1703.10722 , 2017.<br>[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen<br>Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint<br>arXiv:1703.03130 , 2017.<br>[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task<br>sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.<br>[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-<br>based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.</p>',\n",
       "  \"<br>[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang<br>Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine<br>translation system: Bridging the gap between human and machine translation. arXiv preprint<br>arXiv:1609.08144 , 2016.<br>[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with<br>fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.<br>[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate<br>shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume<br>1: Long Papers) , pages 434–443. ACL, August 2013.</p><h1 id='4' style='font-size:20px'>Input-Input Layer5</h1>\",\n",
       "  '<br>all positions in the decoder up to and including that position. We need to prevent leftward<br>information flow in the decoder to preserve the auto-regressive property. We implement this<br>inside of scaled dot-product attention by masking out (setting to −∞ ) all values in the input<br>of the softmax which correspond to illegal connections. See Figure 2.</p><br>',\n",
       "  '<br>and interpretable tree annotation. In Proceedings of the 21st International Conference on<br>Computational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July<br>2006.<br>[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv<br>preprint arXiv:1608.05859 , 2016.<br>[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words<br>with subword units. arXiv preprint arXiv:1508.07909 , 2015.<br>[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,<br>and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts<br>layer. arXiv preprint arXiv:1701.06538 , 2017.<br>[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-<br>nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine<br>Learning Research , 15(1):1929–1958, 2014.',\n",
       "  '<br>our model establishes a new single-model state-of-the-art BLEU score of 41.8 after<br>training for 3.5 days on eight GPUs, a small fraction of the training costs of the<br>best models from the literature. We show that the Transformer generalizes well to<br>other tasks by applying it successfully to English constituency parsing both with<br>large and limited training data.</p>',\n",
       "  '<br>per-word perplexities.</p>',\n",
       "  '<br>tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.<br>[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint<br>arXiv:1308.0850 , 2013.<br>[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-<br>age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern<br>Recognition , pages 770–778, 2016.<br>[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in<br>recurrent nets: the difficulty of learning long-term dependencies, 2001.<br>[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,<br>9(8):1735–1780, 1997.<br>[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations<br>across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural<br>Language Processing , pages 832–841. ACL, August 2009.',\n",
       "  \"<p id='0' data-category='paragraph' style='font-size:14px'>[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,<br>and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical<br>machine translation. CoRR , abs/1406.1078, 2014.<br>[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv<br>preprint arXiv:1610.02357 , 2016.<br>[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation<br>of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.<br>[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural<br>network grammars. In Proc. of NAACL , 2016.<br>[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-<br>tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\",\n",
       "  \"<p id='101' data-category='paragraph' style='font-size:16px'>On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)<br>in Table 2) outperforms the best previously reported models (including ensembles) by more than 2 . 0<br>BLEU, establishing a new state-of-the-art BLEU score of 28 . 4 . The configuration of this model is<br>listed in the bottom line of Table 3. Training took 3 . 5 days on 8 P100 GPUs. Even our base model<br>surpasses all previously published models and ensembles, at a fraction of the training cost of any of<br>the competitive models.</p><br>\",\n",
       "  \"<p id='102' data-category='paragraph' style='font-size:14px'>On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41 . 0 ,<br>outperforming all of the previously published single models, at less than 1 / 4 the training cost of the<br>previous state-of-the-art model. The Transformer (big) model trained for English-to-French used<br>dropout rate P drop = 0 . 1 , instead of 0 . 3 .</p><br><p id='103' data-category='paragraph' style='font-size:16px'>For the base models, we used a single model obtained by averaging the last 5 checkpoints, which<br>were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We<br>used beam search with a beam size of 4 and length penalty α = 0 . 6 [ 38 ]. These hyperparameters<br>were chosen after experimentation on the development set. We set the maximum output length during<br>inference to input length + 50 , but terminate early when possible [38].</p><br>\",\n",
       "  \"<p id='104' data-category='paragraph' style='font-size:16px'>Table 2 summarizes our results and compares our translation quality and training costs to other model<br>architectures from the literature. We estimate the number of floating point operations used to train a<br>model by multiplying the training time, the number of GPUs used, and an estimate of the sustained<br>single-precision floating-point capacity of each GPU 5 .</p><h1 id='105' style='font-size:20px'>6.2 Model Variations</h1><br><p id='106' data-category='paragraph' style='font-size:18px'>To evaluate the importance of different components of the Transformer, we varied our base model<br>in different ways, measuring the change in performance on English-to-German translation on the</p><p id='107' data-category='paragraph' style='font-size:14px'>5 We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.</p>\",\n",
       "  \"<p id='109' data-category='paragraph' style='font-size:16px'>Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base<br>model. All metrics are on the English-to-German translation development set, newstest2013. Listed<br>perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\",\n",
       "  \"<p id='111' data-category='paragraph' style='font-size:16px'>development set, newstest2013. We used beam search as described in the previous section, but no<br>checkpoint averaging. We present these results in Table 3.</p><br><p id='112' data-category='paragraph' style='font-size:16px'>In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,<br>keeping the amount of computation constant, as described in Section 3.2.2. While single-head<br>attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.</p><br>\",\n",
       "  \"<p id='113' data-category='paragraph' style='font-size:14px'>In Table 3 rows (B), we observe that reducing the attention key size d k hurts model quality. This<br>suggests that determining compatibility is not easy and that a more sophisticated compatibility<br>function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,<br>bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our<br>sinusoidal positional encoding with learned positional embeddings [ 9 ], and observe nearly identical<br>results to the base model.</p><h1 id='114' style='font-size:20px'>6.3 English Constituency Parsing</h1><br>\",\n",
       "  \"<p id='115' data-category='paragraph' style='font-size:16px'>To evaluate if the Transformer can generalize to other tasks we performed experiments on English<br>constituency parsing. This task presents specific challenges: the output is subject to strong structural<br>constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence<br>models have not been able to attain state-of-the-art results in small-data regimes [37].</p><br><p id='116' data-category='paragraph' style='font-size:14px'>We trained a 4-layer transformer with d model = 1024 on the Wall Street Journal (WSJ) portion of the<br>Penn Treebank [ 25 ], about 40K training sentences. We also trained it in a semi-supervised setting,<br>using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences<br>[ 37 ]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens<br>for the semi-supervised setting.</p><br>\",\n",
       "  \"<p id='117' data-category='paragraph' style='font-size:16px'>We performed only a small number of experiments to select the dropout, both attention and residual<br>(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters<br>remained unchanged from the English-to-German base translation model. During inference, we</p>\",\n",
       "  \"<p id='119' data-category='paragraph' style='font-size:18px'>Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23<br>of WSJ)</p>\",\n",
       "  \"<p id='121' data-category='paragraph' style='font-size:14px'>increased the maximum output length to input length + 300 . We used a beam size of 21 and α = 0 . 3<br>for both WSJ only and the semi-supervised setting.</p><br><p id='122' data-category='paragraph' style='font-size:18px'>Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-<br>prisingly well, yielding better results than all previously reported models with the exception of the<br>Recurrent Neural Network Grammar [8].</p><br><p id='123' data-category='paragraph' style='font-size:18px'>In contrast to RNN sequence-to-sequence models [ 37 ], the Transformer outperforms the Berkeley-<br>Parser [29] even when training only on the WSJ training set of 40K sentences.</p><br><h1 id='124' style='font-size:20px'>7 Conclusion</h1><br>\",\n",
       "  \"<p id='125' data-category='paragraph' style='font-size:18px'>In this work, we presented the Transformer, the first sequence transduction model based entirely on<br>attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with<br>multi-headed self-attention.</p><br><p id='126' data-category='paragraph' style='font-size:18px'>For translation tasks, the Transformer can be trained significantly faster than architectures based<br>on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014<br>English-to-French translation tasks, we achieve a new state of the art. In the former task our best<br>model outperforms even all previously reported ensembles.</p><br>\",\n",
       "  \"<p id='127' data-category='paragraph' style='font-size:18px'>We are excited about the future of attention-based models and plan to apply them to other tasks. We<br>plan to extend the Transformer to problems involving input and output modalities other than text and<br>to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs<br>such as images, audio and video. Making generation less sequential is another research goals of ours.</p><br><p id='128' data-category='paragraph' style='font-size:16px'>The code we used to train and evaluate our models is available at https://github.com/<br>tensorflow/tensor2tensor .</p><br><p id='129' data-category='paragraph' style='font-size:18px'>Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful<br>comments, corrections and inspiration.</p><br><p id='130' data-category='paragraph' style='font-size:20px'>References</p><br>\",\n",
       "  \"<p id='13' data-category='paragraph' style='font-size:16px'>Recurrent models typically factor computation along the symbol positions of the input and output<br>sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden<br>states h t , as a function of the previous hidden state h t − 1 and the input for position t . This inherently<br>sequential nature precludes parallelization within training examples, which becomes critical at longer<br>sequence lengths, as memory constraints limit batching across examples. Recent work has achieved<br>significant improvements in computational efficiency through factorization tricks [ 21 ] and conditional<br>computation [ 32 ], while also improving model performance in case of the latter. The fundamental<br>constraint of sequential computation, however, remains.</p><br>\",\n",
       "  \"<p id='130' data-category='paragraph' style='font-size:20px'>References</p><br><p id='131' data-category='paragraph' style='font-size:16px'>[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint<br>arXiv:1607.06450 , 2016.<br>[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly<br>learning to align and translate. CoRR , abs/1409.0473, 2014.<br>[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural<br>machine translation architectures. CoRR , abs/1703.03906, 2017.<br>[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine<br>reading. arXiv preprint arXiv:1601.06733 , 2016.</p>\",\n",
       "  '<p id=\\'14\\' data-category=\\'paragraph\\' style=\\'font-size:16px\\'>The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad></p><p id=\\'15\\' data-category=\\'paragraph\\' style=\\'font-size:18px\\'>The Law will never be perfect Layer5 should be just - this is what we are missing , in my opinion . <EOS> <pad><br>, but its application<br>Input-Input</p><figure><img id=\\'16\\' style=\\'font-size:16px\\' alt=\"The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>\\nThe Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>\" data-coord=\"top-left:(233,830); bottom-right:(1052,1227)\" /></figure>',\n",
       "  \"<p id='14' data-category='paragraph' style='font-size:18px'>Attention mechanisms have become an integral part of compelling sequence modeling and transduc-<br>tion models in various tasks, allowing modeling of dependencies without regard to their distance in<br>the input or output sequences [ 2 , 19 ]. In all but a few cases [ 27 ], however, such attention mechanisms<br>are used in conjunction with a recurrent network.</p><br><p id='15' data-category='paragraph' style='font-size:18px'>In this work we propose the Transformer, a model architecture eschewing recurrence and instead<br>relying entirely on an attention mechanism to draw global dependencies between input and output.<br>The Transformer allows for significantly more parallelization and can reach a new state of the art in<br>translation quality after being trained for as little as twelve hours on eight P100 GPUs.</p><h1 id='16' style='font-size:20px'>2 Background</h1>\",\n",
       "  \"<p id='17' data-category='paragraph' style='font-size:18px'>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU<br>[ 16 ], ByteNet [ 18 ] and ConvS2S [ 9 ], all of which use convolutional neural networks as basic building<br>block, computing hidden representations in parallel for all input and output positions. In these models,<br>the number of operations required to relate signals from two arbitrary input or output positions grows<br>in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes<br>it more difficult to learn dependencies between distant positions [ 12 ]. In the Transformer this is<br>reduced to a constant number of operations, albeit at the cost of reduced effective resolution due<br>to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as<br>described in section 3.2.</p><br>\",\n",
       "  \"<p id='18' data-category='paragraph' style='font-size:18px'>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions<br>of a single sequence in order to compute a representation of the sequence. Self-attention has been<br>used successfully in a variety of tasks including reading comprehension, abstractive summarization,<br>textual entailment and learning task-independent sentence representations [4, 27, 28, 22].</p><br><p id='19' data-category='paragraph' style='font-size:18px'>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-<br>aligned recurrence and have been shown to perform well on simple-language question answering and<br>language modeling tasks [34].</p><br>\",\n",
       "  \"<p id='2' data-category='paragraph' style='font-size:14px'>[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated<br>corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.<br>[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In<br>Proceedings of the Human Language Technology Conference of the NAACL, Main Conference ,<br>pages 152–159. ACL, June 2006.<br>[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention<br>model. In Empirical Methods in Natural Language Processing , 2016.<br>[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive<br>summarization. arXiv preprint arXiv:1705.04304 , 2017.<br>[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,<br>and interpretable tree annotation. In Proceedings of the 21st International Conference on\",\n",
       "  \"<p id='20' data-category='paragraph' style='font-size:18px'>To the best of our knowledge, however, the Transformer is the first transduction model relying<br>entirely on self-attention to compute representations of its input and output without using sequence-<br>aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate<br>self-attention and discuss its advantages over models such as [17, 18] and [9].</p><h1 id='21' style='font-size:20px'>3 Model Architecture</h1>\",\n",
       "  '<p id=\\'22\\' data-category=\\'paragraph\\' style=\\'font-size:14px\\'>Most competitive neural sequence transduction models have an encoder-decoder structure [ 5 , 2 , 35 ].<br>Here, the encoder maps an input sequence of symbol representations ( x 1 , ..., x n ) to a sequence<br>of continuous representations z = ( z 1 , ..., z n ) . Given z , the decoder then generates an output<br>sequence ( y 1 , ..., y m ) of symbols one element at a time. At each step the model is auto-regressive<br>[10], consuming the previously generated symbols as additional input when generating the next.</p><figure><img id=\\'24\\' alt=\"\" data-coord=\"top-left:(402,143); bottom-right:(872,833)\" /></figure><br><caption id=\\'25\\' style=\\'font-size:16px\\'>Figure 1: The Transformer - model architecture.</caption>',\n",
       "  \"<p id='26' data-category='paragraph' style='font-size:16px'>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully<br>connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,<br>respectively.</p><br><h1 id='27' style='font-size:20px'>3.1 Encoder and Decoder Stacks</h1>\",\n",
       "  \"<p id='28' data-category='paragraph' style='font-size:14px'>Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two<br>sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-<br>wise fully connected feed-forward network. We employ a residual connection [ 11 ] around each of<br>the two sub-layers, followed by layer normalization [ 1 ]. That is, the output of each sub-layer is<br>LayerNorm( x + Sublayer( x )) , where Sublayer( x ) is the function implemented by the sub-layer<br>itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding<br>layers, produce outputs of dimension d model = 512 .</p>\",\n",
       "  \"<p id='29' data-category='paragraph' style='font-size:14px'>Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two<br>sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head<br>attention over the output of the encoder stack. Similar to the encoder, we employ residual connections<br>around each of the sub-layers, followed by layer normalization. We also modify the self-attention<br>sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This<br>masking, combined with fact that the output embeddings are offset by one position, ensures that the<br>predictions for position i can depend only on the known outputs at positions less than i .</p><br><h1 id='30' style='font-size:20px'>3.2 Attention</h1><br>\",\n",
       "  '<p id=\\'31\\' data-category=\\'paragraph\\' style=\\'font-size:16px\\'>An attention function can be described as mapping a query and a set of key-value pairs to an output,<br>where the query, keys, values, and output are all vectors. The output is computed as a weighted sum</p><h1 id=\\'33\\' style=\\'font-size:20px\\'>Scaled Dot-Product Attention</h1><figure><img id=\\'34\\' alt=\"\" data-coord=\"top-left:(351,196); bottom-right:(505,465)\" /></figure><br><h1 id=\\'35\\' style=\\'font-size:20px\\'>Multi-Head Attention</h1><figure><img id=\\'36\\' alt=\"\" data-coord=\"top-left:(715,168); bottom-right:(975,505)\" /></figure><p id=\\'37\\' data-category=\\'paragraph\\' style=\\'font-size:20px\\'>Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several<br>attention layers running in parallel.</p>',\n",
       "  '<p id=\\'38\\' data-category=\\'paragraph\\' style=\\'font-size:20px\\'>of the values, where the weight assigned to each value is computed by a compatibility function of the<br>query with the corresponding key.</p><br><h1 id=\\'39\\' style=\\'font-size:22px\\'>3.2.1 Scaled Dot-Product Attention</h1><p id=\\'40\\' data-category=\\'paragraph\\' style=\\'font-size:18px\\'>We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of<br>queries and keys of dimension d k , and values of dimension d v . We compute the dot products of the<br>query with all keys, divide each by √ d k , and apply a softmax function to obtain the weights on the<br>values.</p><br><p id=\\'41\\' data-category=\\'paragraph\\' style=\\'font-size:18px\\'>In practice, we compute the attention function on a set of queries simultaneously, packed together<br>into a matrix Q . The keys and values are also packed together into matrices K and V . We compute<br>the matrix of outputs as:</p>',\n",
       "  \"<p id='42' data-category='equation' style='font-size:14px'>QK T<br>Attention( Q, K, V ) = softmax( √ d k<br>) V (1)</p><p id='43' data-category='paragraph' style='font-size:18px'>The two most commonly used attention functions are additive attention [ 2 ], and dot-product (multi-<br>plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor<br>of √ d k<br>1 . Additive attention computes the compatibility function using a feed-forward network with<br>a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is<br>much faster and more space-efficient in practice, since it can be implemented using highly optimized<br>matrix multiplication code.</p>\",\n",
       "  \"<p id='44' data-category='paragraph' style='font-size:16px'>While for small values of d k the two mechanisms perform similarly, additive attention outperforms<br>dot product attention without scaling for larger values of d k [ 3 ]. We suspect that for large values of<br>d k , the dot products grow large in magnitude, pushing the softmax function into regions where it has<br>extremely small gradients 4 . To counteract this effect, we scale the dot products by √ d k<br>1 .</p><br><h1 id='45' style='font-size:22px'>3.2.2 Multi-Head Attention</h1>\",\n",
       "  \"<p id='46' data-category='paragraph' style='font-size:16px'>Instead of performing a single attention function with d model -dimensional keys, values and queries,<br>we found it beneficial to linearly project the queries, keys and values h times with different, learned<br>linear projections to d k , d k and d v dimensions, respectively. On each of these projected versions of<br>queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional</p><p id='47' data-category='paragraph' style='font-size:14px'>4 To illustrate why the dot products get large, assume that the components of q and k are independent random<br>d k q i k i , has mean 0 and variance d k .<br>variables with mean 0 and variance 1 . Then their dot product, q · k = � i =1</p><p id='49' data-category='paragraph' style='font-size:20px'>output values. These are concatenated and once again projected, resulting in the final values, as<br>depicted in Figure 2.</p><br>\",\n",
       "  \"<p id='5' data-category='paragraph' style='font-size:16px'>Illia Polosukhin ∗ ‡<br>illia.polosukhin@gmail.com</p><h1 id='6' style='font-size:18px'>Abstract</h1>\",\n",
       "  \"<p id='50' data-category='paragraph' style='font-size:20px'>Multi-head attention allows the model to jointly attend to information from different representation<br>subspaces at different positions. With a single attention head, averaging inhibits this.</p><p id='51' data-category='equation' style='font-size:14px'>MultiHead( Q, K, V ) = Concat(head 1 , ..., head h ) W O<br>Q K , V W i )<br>V<br>where head i = Attention( QW i , KW i</p><p id='52' data-category='paragraph' style='font-size:14px'>Q R d model × d k , W i ∈ R d model × d k , W i ∈ R d model × d v<br>Where the projections are parameter matrices W i ∈<br>K<br>V<br>and W O ∈ R hd v × d model .</p><br>\",\n",
       "  \"<p id='53' data-category='paragraph' style='font-size:16px'>In this work we employ h = 8 parallel attention layers, or heads. For each of these we use<br>d k = d v = d model /h = 64 . Due to the reduced dimension of each head, the total computational cost<br>is similar to that of single-head attention with full dimensionality.</p><br><h1 id='54' style='font-size:22px'>3.2.3 Applications of Attention in our Model</h1><br><h1 id='55' style='font-size:20px'>The Transformer uses multi-head attention in three different ways:</h1><br>\",\n",
       "  '<p id=\\'56\\' data-category=\\'paragraph\\' style=\\'font-size:20px\\'>• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,<br>and the memory keys and values come from the output of the encoder. This allows every<br>position in the decoder to attend over all positions in the input sequence. This mimics the<br>typical encoder-decoder attention mechanisms in sequence-to-sequence models such as<br>[38, 2, 9].<br>• The encoder contains self-attention layers. In a self-attention layer all of the keys, values<br>and queries come from the same place, in this case, the output of the previous layer in the<br>encoder. Each position in the encoder can attend to all positions in the previous layer of the<br>encoder.<br>• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to<br>all positions in the decoder up to and including that position. We need to prevent leftward',\n",
       "  \"<p id='57' data-category='paragraph' style='font-size:22px'>3.3 Position-wise Feed-Forward Networks</p><br><p id='58' data-category='paragraph' style='font-size:20px'>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully<br>connected feed-forward network, which is applied to each position separately and identically. This<br>consists of two linear transformations with a ReLU activation in between.</p><p id='59' data-category='equation' style='font-size:14px'>FFN( x ) = max(0 , xW 1 + b 1 ) W 2 + b 2</p><br><caption id='60' style='font-size:20px'>(2)</caption>\",\n",
       "  \"<p id='61' data-category='paragraph' style='font-size:18px'>While the linear transformations are the same across different positions, they use different parameters<br>from layer to layer. Another way of describing this is as two convolutions with kernel size 1.<br>The dimensionality of input and output is d model = 512 , and the inner-layer has dimensionality<br>d ff = 2048 .</p><h1 id='62' style='font-size:22px'>3.4 Embeddings and Softmax</h1>\",\n",
       "  \"<p id='63' data-category='paragraph' style='font-size:18px'>Similarly to other sequence transduction models, we use learned embeddings to convert the input<br>tokens and output tokens to vectors of dimension d model . We also use the usual learned linear transfor-<br>mation and softmax function to convert the decoder output to predicted next-token probabilities. In<br>our model, we share the same weight matrix between the two embedding layers and the pre-softmax<br>linear transformation, similar to [ 30 ]. In the embedding layers, we multiply those weights by √ d model .</p>\",\n",
       "  \"<p id='65' data-category='paragraph' style='font-size:18px'>Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations<br>for different layer types. n is the sequence length, d is the representation dimension, k is the kernel<br>size of convolutions and r the size of the neighborhood in restricted self-attention.</p><table id='66' style='font-size:14px'><tr><td>Layer Type</td><td>Complexity per Layer</td><td>Sequential Operations</td><td>Maximum Path Length</td></tr><tr><td>Self-Attention</td><td>O ( n 2 · d )</td><td>O (1)</td><td>O (1)</td></tr><tr><td>Recurrent</td><td>O ( n · d 2 )</td><td>O ( n )</td><td>O ( n )</td></tr><tr><td>Convolutional</td><td>O ( k · n · d 2 )</td><td>O (1)</td><td>O ( log k ( n ))</td></tr><tr><td>Self-Attention (restricted)</td><td>O ( r · n · d )</td><td>O (1)</td><td>O ( n/r )</td></tr></table><h1 id='67' style='font-size:20px'>3.5 Positional Encoding</h1>\",\n",
       "  '<p id=\\'68\\' data-category=\\'paragraph\\' style=\\'font-size:18px\\'>Since our model contains no recurrence and no convolution, in order for the model to make use of the<br>order of the sequence, we must inject some information about the relative or absolute position of the<br>tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the<br>bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model<br>as the embeddings, so that the two can be summed. There are many choices of positional encodings,<br>learned and fixed [9].</p><br><p id=\\'69\\' data-category=\\'paragraph\\' style=\\'font-size:18px\\'>In this work, we use sine and cosine functions of different frequencies:</p><p id=\\'70\\' data-category=\\'equation\\' style=\\'font-size:14px\\'>PE ( pos, 2 i ) = sin ( pos/ 10000 2 i/d model )<br>PE ( pos, 2 i +1) = cos ( pos/ 10000 2 i/d model )</p>',\n",
       "  \"<p id='7' data-category='paragraph' style='font-size:16px'>The dominant sequence transduction models are based on complex recurrent or<br>convolutional neural networks that include an encoder and a decoder. The best<br>performing models also connect the encoder and decoder through an attention<br>mechanism. We propose a new simple network architecture, the Transformer,<br>based solely on attention mechanisms, dispensing with recurrence and convolutions<br>entirely. Experiments on two machine translation tasks show these models to<br>be superior in quality while being more parallelizable and requiring significantly<br>less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-<br>to-German translation task, improving over the existing best results, including<br>ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,<br>our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\",\n",
       "  \"<p id='71' data-category='paragraph' style='font-size:16px'>where pos is the position and i is the dimension. That is, each dimension of the positional encoding<br>corresponds to a sinusoid. The wavelengths form a geometric progression from 2 π to 10000 · 2 π . We<br>chose this function because we hypothesized it would allow the model to easily learn to attend by<br>relative positions, since for any fixed offset k , PE pos + k can be represented as a linear function of<br>PE pos .</p><br><p id='72' data-category='paragraph' style='font-size:18px'>We also experimented with using learned positional embeddings [ 9 ] instead, and found that the two<br>versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version<br>because it may allow the model to extrapolate to sequence lengths longer than the ones encountered<br>during training.</p><br><h1 id='73' style='font-size:22px'>4 Why Self-Attention</h1>\",\n",
       "  \"<p id='74' data-category='paragraph' style='font-size:16px'>In this section we compare various aspects of self-attention layers to the recurrent and convolu-<br>tional layers commonly used for mapping one variable-length sequence of symbol representations<br>( x 1 , ..., x n ) to another sequence of equal length ( z 1 , ..., z n ) , with x i , z i ∈ R d , such as a hidden<br>layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we<br>consider three desiderata.</p><br><p id='75' data-category='paragraph' style='font-size:18px'>One is the total computational complexity per layer. Another is the amount of computation that can<br>be parallelized, as measured by the minimum number of sequential operations required.</p><br>\",\n",
       "  \"<p id='76' data-category='paragraph' style='font-size:18px'>The third is the path length between long-range dependencies in the network. Learning long-range<br>dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the<br>ability to learn such dependencies is the length of the paths forward and backward signals have to<br>traverse in the network. The shorter these paths between any combination of positions in the input<br>and output sequences, the easier it is to learn long-range dependencies [ 12 ]. Hence we also compare<br>the maximum path length between any two input and output positions in networks composed of the<br>different layer types.</p><br>\",\n",
       "  \"<p id='77' data-category='paragraph' style='font-size:18px'>As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially<br>executed operations, whereas a recurrent layer requires O ( n ) sequential operations. In terms of<br>computational complexity, self-attention layers are faster than recurrent layers when the sequence</p>\",\n",
       "  \"<p id='79' data-category='paragraph' style='font-size:16px'>length n is smaller than the representation dimensionality d , which is most often the case with<br>sentence representations used by state-of-the-art models in machine translations, such as word-piece<br>[ 38 ] and byte-pair [ 31 ] representations. To improve computational performance for tasks involving<br>very long sequences, self-attention could be restricted to considering only a neighborhood of size r in<br>the input sequence centered around the respective output position. This would increase the maximum<br>path length to O ( n/r ) . We plan to investigate this approach further in future work.</p><br>\",\n",
       "  \"<p id='8' data-category='paragraph' style='font-size:14px'>∗ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started<br>the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and<br>has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head<br>attention and the parameter-free position representation and became the other person involved in nearly every<br>detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and<br>tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and<br>efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and<br>implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating<br>our research.</p><br>\",\n",
       "  \"<p id='80' data-category='paragraph' style='font-size:16px'>A single convolutional layer with kernel width k < n does not connect all pairs of input and output<br>positions. Doing so requires a stack of O ( n/k ) convolutional layers in the case of contiguous kernels,<br>or O ( log k ( n )) in the case of dilated convolutions [ 18 ], increasing the length of the longest paths<br>between any two positions in the network. Convolutional layers are generally more expensive than<br>recurrent layers, by a factor of k . Separable convolutions [ 6 ], however, decrease the complexity<br>considerably, to O ( k · n · d + n · d 2 ) . Even with k = n , however, the complexity of a separable<br>convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,<br>the approach we take in our model.</p><br>\",\n",
       "  \"<p id='81' data-category='paragraph' style='font-size:18px'>As side benefit, self-attention could yield more interpretable models. We inspect attention distributions<br>from our models and present and discuss examples in the appendix. Not only do individual attention<br>heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic<br>and semantic structure of the sentences.</p><h1 id='82' style='font-size:22px'>5 Training</h1><h1 id='83' style='font-size:18px'>This section describes the training regime for our models.</h1><h1 id='84' style='font-size:20px'>5.1 Training Data and Batching</h1><br>\",\n",
       "  \"<p id='85' data-category='paragraph' style='font-size:18px'>We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million<br>sentence pairs. Sentences were encoded using byte-pair encoding [ 3 ], which has a shared source-<br>target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT<br>2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece<br>vocabulary [ 38 ]. Sentence pairs were batched together by approximate sequence length. Each training<br>batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000<br>target tokens.</p><h1 id='86' style='font-size:20px'>5.2 Hardware and Schedule</h1><br>\",\n",
       "  \"<p id='87' data-category='paragraph' style='font-size:18px'>We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using<br>the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We<br>trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the<br>bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps<br>(3.5 days).</p><h1 id='88' style='font-size:20px'>5.3 Optimizer</h1><p id='89' data-category='paragraph' style='font-size:14px'>We used the Adam optimizer [ 20 ] with β 1 = 0 . 9 , β 2 = 0 . 98 and ϵ = 10 − 9 . We varied the learning<br>rate over the course of training, according to the formula:</p><p id='90' data-category='equation' style='font-size:14px'>lrate = d − 0 . 5 · min( step _ num − 0 . 5 , step _ num · warmup _ steps − 1 . 5 ) (3)<br>model</p>\",\n",
       "  \"<p id='9' data-category='paragraph' style='font-size:14px'>† Work performed while at Google Brain.<br>‡ Work performed while at Google Research.</p><p id='10' data-category='paragraph' style='font-size:14px'>31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.</p><h1 id='11' style='font-size:20px'>1 Introduction</h1><p id='12' data-category='paragraph' style='font-size:18px'>Recurrent neural networks, long short-term memory [ 13 ] and gated recurrent [ 7 ] neural networks<br>in particular, have been firmly established as state of the art approaches in sequence modeling and<br>transduction problems such as language modeling and machine translation [ 35 , 2 , 5 ]. Numerous<br>efforts have since continued to push the boundaries of recurrent language models and encoder-decoder<br>architectures [38, 24, 15].</p><br>\",\n",
       "  \"<p id='91' data-category='paragraph' style='font-size:16px'>This corresponds to increasing the learning rate linearly for the first warmup _ steps training steps,<br>and decreasing it thereafter proportionally to the inverse square root of the step number. We used<br>warmup _ steps = 4000 .</p><h1 id='92' style='font-size:20px'>5.4 Regularization</h1><br><p id='93' data-category='paragraph' style='font-size:18px'>We employ three types of regularization during training:</p>\",\n",
       "  \"<p id='95' data-category='paragraph' style='font-size:18px'>Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the<br>English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.</p>\",\n",
       "  \"<p id='97' data-category='paragraph' style='font-size:16px'>Residual Dropout We apply dropout [ 33 ] to the output of each sub-layer, before it is added to the<br>sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the<br>positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of<br>P drop = 0 . 1 .</p><p id='98' data-category='paragraph' style='font-size:16px'>Label Smoothing During training, we employed label smoothing of value ϵ ls = 0 . 1 [ 36 ]. This<br>hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.</p><h1 id='99' style='font-size:22px'>6 Results</h1><h1 id='100' style='font-size:20px'>6.1 Machine Translation</h1>\",\n",
       "  '<pad>\\nThe Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>\\nInput-Input Layer5\\nThe Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>\\nThe Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>\" data-coord=\"top-left:(235,351); bottom-right:(1063,1269)\" /></figure><br><p id=\\'11\\' data-category=\\'paragraph\\' style=\\'font-size:14px\\'>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:<br>Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5<br>and 6. Note that the attentions are very sharp for this word.</p><h1 id=\\'13\\' style=\\'font-size:20px\\'>Input-Input Layer5</h1>',\n",
       "  '<pad> <pad> <pad> <pad> <pad> <pad>\\nIt is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad>\" data-coord=\"top-left:(235,209); bottom-right:(1062,638)\" /></figure><br><caption id=\\'7\\' style=\\'font-size:14px\\'>Figure 3: An example of the attention mechanism following long-distance dependencies in the<br>encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of<br>the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for<br>the word ‘making’. Different colors represent different heads. Best viewed in color.</caption><h1 id=\\'9\\' style=\\'font-size:20px\\'>Input-Input Layer5</h1><figure><img id=\\'10\\' style=\\'font-size:16px\\' alt=\"The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS>',\n",
       "  '<pad>\" data-coord=\"top-left:(233,830); bottom-right:(1052,1227)\" /></figure><p id=\\'17\\' data-category=\\'paragraph\\' style=\\'font-size:14px\\'>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the<br>sentence. We give two such examples above, from two different heads from the encoder self-attention<br>at layer 5 of 6. The heads clearly learned to perform different tasks.</p>',\n",
       "  '<table id=\\'110\\' style=\\'font-size:14px\\'><tr><td></td><td>N</td><td>d model</td><td>d ff</td><td>h</td><td>d k</td><td>d v</td><td>P drop</td><td>ϵ ls</td><td>train steps</td><td>PPL (dev)</td><td>BLEU (dev)</td><td>params × 10 6</td></tr><tr><td>base</td><td>6</td><td>512</td><td>2048</td><td>8</td><td>64</td><td>64</td><td>0.1</td><td>0.1</td><td>100K</td><td>4.92</td><td>25.8</td><td>65</td></tr><tr><td rowspan=\"4\">(A)</td><td></td><td></td><td></td><td>1</td><td>512</td><td>512</td><td></td><td></td><td></td><td>5.29</td><td>24.9</td><td></td></tr><tr><td></td><td></td><td></td><td>4</td><td>128</td><td>128</td><td></td><td></td><td></td><td>5.00</td><td>25.5</td><td></td></tr><tr><td></td><td></td><td></td><td>16</td><td>32</td><td>32</td><td></td><td></td><td></td><td>4.91</td><td>25.8</td><td></td></tr><tr><td></td><td></td><td></td><td>32</td><td>16</td><td>16</td><td></td><td></td><td></td><td>5.01</td><td>25.4</td><td></td></tr>',\n",
       "  \"<table id='120' style='font-size:18px'><tr><td>Parser</td><td>Training</td><td>WSJ 23 F1</td></tr><tr><td>Vinyals & Kaiser el al. (2014) [37]</td><td>WSJ only, discriminative</td><td>88.3</td></tr><tr><td>Petrov et al. (2006) [29]</td><td>WSJ only, discriminative</td><td>90.4</td></tr><tr><td>Zhu et al. (2013) [40]</td><td>WSJ only, discriminative</td><td>90.4</td></tr><tr><td>Dyer et al. (2016) [8]</td><td>WSJ only, discriminative</td><td>91.7</td></tr><tr><td>Transformer (4 layers)</td><td>WSJ only, discriminative</td><td>91.3</td></tr><tr><td>Zhu et al. (2013) [40]</td><td>semi-supervised</td><td>91.3</td></tr><tr><td>Huang & Harper (2009) [14]</td><td>semi-supervised</td><td>91.3</td></tr><tr><td>McClosky et al. (2006) [26]</td><td>semi-supervised</td><td>92.1</td></tr><tr><td>Vinyals & Kaiser el al. (2014) [37]</td><td>semi-supervised</td><td>92.1</td></tr><tr><td>Transformer (4 layers)</td><td>semi-supervised</td><td>92.7</td></tr>\",\n",
       "  '<table id=\\'96\\' style=\\'font-size:14px\\'><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">BLEU</td><td colspan=\"2\">Training Cost (FLOPs)</td></tr><tr><td>EN-DE</td><td>EN-FR</td><td>EN-DE</td><td>EN-FR</td></tr><tr><td>ByteNet [18]</td><td>23.75</td><td></td><td></td><td></td></tr><tr><td>Deep-Att + PosUnk [39]</td><td></td><td>39.2</td><td></td><td>1 . 0 · 10 20</td></tr><tr><td>GNMT + RL [38]</td><td>24.6</td><td>39.92</td><td>2 . 3 · 10 19</td><td>1 . 4 · 10 20</td></tr><tr><td>ConvS2S [9]</td><td>25.16</td><td>40.46</td><td>9 . 6 · 10 18</td><td>1 . 5 · 10 20</td></tr><tr><td>MoE [32]</td><td>26.03</td><td>40.56</td><td>2 . 0 · 10 19</td><td>1 . 2 · 10 20</td></tr><tr><td>Deep-Att + PosUnk Ensemble [39]</td><td></td><td>40.4</td><td></td><td>8 . 0 · 10 20</td></tr><tr><td>GNMT + RL Ensemble [38]</td><td>26.30</td><td>41.16</td><td>1 . 8 · 10 20</td><td>1 . 1 · 10 21</td></tr><tr><td>ConvS2S Ensemble [9]</td><td>26.36</td><td>41.29</td><td>7 . 7 · 10 19</td><td>1 . 2 · 10 21</td></tr>',\n",
       "  '<tr><td rowspan=\"2\">(B)</td><td></td><td></td><td></td><td></td><td>16</td><td></td><td></td><td></td><td></td><td>5.16</td><td>25.1</td><td>58</td></tr><tr><td></td><td></td><td></td><td></td><td>32</td><td></td><td></td><td></td><td></td><td>5.01</td><td>25.4</td><td>60</td></tr><tr><td rowspan=\"7\">(C)</td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>6.11</td><td>23.7</td><td>36</td></tr><tr><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.19</td><td>25.3</td><td>50</td></tr><tr><td>8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.88</td><td>25.5</td><td>80</td></tr><tr><td></td><td>256</td><td></td><td></td><td>32</td><td>32</td><td></td><td></td><td></td><td>5.75</td><td>24.5</td><td>28</td></tr><tr><td></td><td>1024</td><td></td><td></td><td>128</td><td>128</td><td></td><td></td><td></td><td>4.66</td><td>26.0</td><td>168</td></tr>',\n",
       "  '<tr><td></td><td></td><td>1024</td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.12</td><td>25.4</td><td>53</td></tr><tr><td></td><td></td><td>4096</td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.75</td><td>26.2</td><td>90</td></tr><tr><td rowspan=\"4\">(D)</td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td></td><td>5.77</td><td>24.6</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td></td><td>4.95</td><td>25.5</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td>4.67</td><td>25.3</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td>5.47</td><td>25.7</td><td></td></tr><tr><td>(E)</td><td colspan=\"9\">positional embedding instead of sinusoids</td><td>4.92</td><td>25.7</td><td></td></tr>',\n",
       "  '<tr><td>Transformer (4 layers)</td><td>semi-supervised</td><td>92.7</td></tr><tr><td>Luong et al. (2015) [23]</td><td>multi-task</td><td>93.0</td></tr><tr><td>Dyer et al. (2016) [8]</td><td>generative</td><td>93.3</td></tr></table>',\n",
       "  '<tr><td>Transformer (base model)</td><td>27.3</td><td>38.1</td><td colspan=\"2\">3 . 3 · 10 18</td></tr><tr><td>Transformer (big)</td><td>28.4</td><td>41.8</td><td colspan=\"2\">2 . 3 · 10 19</td></tr></table>',\n",
       "  '<tr><td>big</td><td>6</td><td>1024</td><td>4096</td><td>16</td><td></td><td></td><td>0.3</td><td></td><td>300K</td><td>4.33</td><td>26.4</td><td>213</td></tr></table>'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    collection_name=\"paper\"\n",
    ")\n",
    "vectorstore.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate function for generate embedding\n",
    "\n",
    "def generate_embeddings(pdf_filepath, embedding_name):\n",
    "    \"\"\"\n",
    "    Generate Embeddings for the given pdf, return 1 if success otherwise return 0\n",
    "    \"\"\"\n",
    "    try:\n",
    "        layzer = UpstageLayoutAnalysisLoader(pdf_filepath, output_type=\"html\")\n",
    "        docs = layzer.load()\n",
    "        # split text into text chunks\n",
    "        text_spliiter = RecursiveCharacterTextSplitter.from_language(\n",
    "            chunk_size=1000, chunk_overlap=100, language=Language.HTML\n",
    "        )\n",
    "        splits = text_spliiter.split_documents(docs)\n",
    "        unique_splits=[]\n",
    "        for split in splits:\n",
    "            if split not in unique_splits:\n",
    "                unique_splits.append(split)\n",
    "        persist_directory=f\"./chroma_db/{embedding_name}/\"\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=unique_splits,\n",
    "            ids=[doc.page_content for doc in unique_splits],\n",
    "            embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "            persist_directory=persist_directory,\n",
    "        )\n",
    "        print(f\"embedding saved in ./chroma_db/{embedding_name}/\")\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(retriever, input_query):\n",
    "    return retriever.invoke(input_query)\n",
    "\n",
    "def get_retriever(embedding_name):\n",
    "    vector_db = Chroma(\n",
    "        persist_directory=f\"./chroma_db/{embedding_name}/\",\n",
    "        embedding_function=UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "    )\n",
    "    return vector_db.as_retriever()\n",
    "\n",
    "def inference(question, embedding_names):\n",
    "    name_list = os.listdir('./chroma_db/')\n",
    "    embedding_names = list(set(embedding_names))\n",
    "    # check if their are any invalid names of embeddings\n",
    "    print(\"[[CHECKING EMBEDDING NAMES...]]\")\n",
    "    for name in embedding_names:\n",
    "        if name not in embedding_names:\n",
    "            raise Exception(f\"{name} embedding not found.\")\n",
    "\n",
    "    print(\"[[LOADING EMBEDDINGS...]]\")\n",
    "    # accumulate retrivers into a single list\n",
    "    retrievers=[]\n",
    "    for name in embedding_names:\n",
    "        retriever = get_retriever(name)\n",
    "        print(retriever)\n",
    "        retrievers.append((retriever, name))\n",
    "    # generate documents\n",
    "    print(\"[[RETRIEVING RELEVANT DOCS...]]\")\n",
    "    context=\"\"\n",
    "    for retriever in retrievers:\n",
    "        result = retrieve(retriever[0], question)\n",
    "        print(result)\n",
    "        #TODO post process result\n",
    "        refined_result=\"\"+f\"from [{retriever[1]}]\"\n",
    "        context+=refined_result+\"\\n\\n\"\n",
    "    \n",
    "    system_msg = SystemMessagePromptTemplate.from_template(\n",
    "        \"You are an assistant for question-answering tasks.\\n\"\n",
    "        \"Use the following pieces of retrieved context to answer the question.\\n\"\n",
    "        \"If you don't know the answer, just say that you don't know.\\n\"\n",
    "        \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    )\n",
    "    human_msg = HumanMessagePromptTemplate.from_template(\n",
    "    \"Question: {question}\\n\\n\" \n",
    "    \"Context: {context}\" \n",
    "    \"Answer:\"\n",
    "    )\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "        [system_msg, human_msg]\n",
    "    )\n",
    "    model = ChatUpstage(api_key=api_key)\n",
    "    chain = chat_prompt | model\n",
    "    output = chain.invoke({'question': question, 'context': context})\n",
    "    return output.content\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[CHECKING EMBEDDING NAMES...]]\n",
      "[[LOADING EMBEDDINGS...]]\n",
      "tags=['Chroma', 'UpstageEmbeddings'] vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x147992620>\n",
      "[[RETRIEVING RELEVANT DOCS...]]\n",
      "[Document(metadata={'total_pages': 15}, page_content='<p id=\\'31\\' data-category=\\'paragraph\\' style=\\'font-size:16px\\'>An attention function can be described as mapping a query and a set of key-value pairs to an output,<br>where the query, keys, values, and output are all vectors. The output is computed as a weighted sum</p><h1 id=\\'33\\' style=\\'font-size:20px\\'>Scaled Dot-Product Attention</h1><figure><img id=\\'34\\' alt=\"\" data-coord=\"top-left:(351,196); bottom-right:(505,465)\" /></figure><br><h1 id=\\'35\\' style=\\'font-size:20px\\'>Multi-Head Attention</h1><figure><img id=\\'36\\' alt=\"\" data-coord=\"top-left:(715,168); bottom-right:(975,505)\" /></figure><p id=\\'37\\' data-category=\\'paragraph\\' style=\\'font-size:20px\\'>Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several<br>attention layers running in parallel.</p>'), Document(metadata={'total_pages': 15}, page_content='<p id=\\'38\\' data-category=\\'paragraph\\' style=\\'font-size:20px\\'>of the values, where the weight assigned to each value is computed by a compatibility function of the<br>query with the corresponding key.</p><br><h1 id=\\'39\\' style=\\'font-size:22px\\'>3.2.1 Scaled Dot-Product Attention</h1><p id=\\'40\\' data-category=\\'paragraph\\' style=\\'font-size:18px\\'>We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of<br>queries and keys of dimension d k , and values of dimension d v . We compute the dot products of the<br>query with all keys, divide each by √ d k , and apply a softmax function to obtain the weights on the<br>values.</p><br><p id=\\'41\\' data-category=\\'paragraph\\' style=\\'font-size:18px\\'>In practice, we compute the attention function on a set of queries simultaneously, packed together<br>into a matrix Q . The keys and values are also packed together into matrices K and V . We compute<br>the matrix of outputs as:</p>'), Document(metadata={'total_pages': 15}, page_content=\"<br><p id='1' data-category='paragraph' style='font-size:18px'>Provided proper attribution is provided, Google hereby grants permission to<br>reproduce the tables and figures in this paper solely for use in journalistic or<br>scholarly works.</p><h1 id='2' style='font-size:20px'>Attention Is All You Need</h1><table id='3' style='font-size:16px'><tr><td>Ashish Vaswani ∗</td><td>Noam Shazeer ∗</td><td>Niki Parmar ∗</td><td>Jakob Uszkoreit ∗</td></tr><tr><td>Google Brain</td><td>Google Brain</td><td>Google Research</td><td>Google Research</td></tr><tr><td>avaswani@google.com</td><td>noam@google.com</td><td>nikip@google.com</td><td>usz@google.com</td></tr></table><table id='4' style='font-size:16px'><tr><td>Llion Jones ∗</td><td>Aidan N. Gomez ∗ †</td><td>Łukasz Kaiser ∗</td></tr><tr><td>Google Research</td><td>University of Toronto</td><td>Google Brain</td></tr><tr><td>llion@google.com</td><td>aidan@cs.toronto.edu</td><td>lukaszkaiser@google.com</td></tr></table>\"), Document(metadata={'total_pages': 15}, page_content=\"<p id='18' data-category='paragraph' style='font-size:18px'>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions<br>of a single sequence in order to compute a representation of the sequence. Self-attention has been<br>used successfully in a variety of tasks including reading comprehension, abstractive summarization,<br>textual entailment and learning task-independent sentence representations [4, 27, 28, 22].</p><br><p id='19' data-category='paragraph' style='font-size:18px'>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-<br>aligned recurrence and have been shown to perform well on simple-language question answering and<br>language modeling tasks [34].</p><br>\")]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Attention is a cognitive process that involves selecting and focusing on relevant stimuli while ignoring irrelevant ones. It allows individuals to concentrate on specific tasks or information, enabling effective processing and understanding.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(\"What is attention?\", ['attention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[CHECKING EMBEDDING NAMES...]]\n",
      "[[LOADING EMBEDDINGS...]]\n",
      "tags=['Chroma', 'UpstageEmbeddings'] vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x107feae30>\n",
      "tags=['Chroma', 'UpstageEmbeddings'] vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x148dbc700>\n",
      "tags=['Chroma', 'UpstageEmbeddings'] vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x1492b6830>\n",
      "[[RETRIEVING RELEVANT DOCS...]]\n",
      "[Document(metadata={'total_pages': 15}, page_content=\"<p id='14' data-category='paragraph' style='font-size:18px'>Attention mechanisms have become an integral part of compelling sequence modeling and transduc-<br>tion models in various tasks, allowing modeling of dependencies without regard to their distance in<br>the input or output sequences [ 2 , 19 ]. In all but a few cases [ 27 ], however, such attention mechanisms<br>are used in conjunction with a recurrent network.</p><br><p id='15' data-category='paragraph' style='font-size:18px'>In this work we propose the Transformer, a model architecture eschewing recurrence and instead<br>relying entirely on an attention mechanism to draw global dependencies between input and output.<br>The Transformer allows for significantly more parallelization and can reach a new state of the art in<br>translation quality after being trained for as little as twelve hours on eight P100 GPUs.</p><h1 id='16' style='font-size:20px'>2 Background</h1>\"), Document(metadata={'total_pages': 15}, page_content=\"<p id='18' data-category='paragraph' style='font-size:18px'>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions<br>of a single sequence in order to compute a representation of the sequence. Self-attention has been<br>used successfully in a variety of tasks including reading comprehension, abstractive summarization,<br>textual entailment and learning task-independent sentence representations [4, 27, 28, 22].</p><br><p id='19' data-category='paragraph' style='font-size:18px'>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-<br>aligned recurrence and have been shown to perform well on simple-language question answering and<br>language modeling tasks [34].</p><br>\"), Document(metadata={'total_pages': 15}, page_content=\"<p id='7' data-category='paragraph' style='font-size:16px'>The dominant sequence transduction models are based on complex recurrent or<br>convolutional neural networks that include an encoder and a decoder. The best<br>performing models also connect the encoder and decoder through an attention<br>mechanism. We propose a new simple network architecture, the Transformer,<br>based solely on attention mechanisms, dispensing with recurrence and convolutions<br>entirely. Experiments on two machine translation tasks show these models to<br>be superior in quality while being more parallelizable and requiring significantly<br>less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-<br>to-German translation task, improving over the existing best results, including<br>ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,<br>our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\"), Document(metadata={'total_pages': 15}, page_content='<p id=\\'56\\' data-category=\\'paragraph\\' style=\\'font-size:20px\\'>• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,<br>and the memory keys and values come from the output of the encoder. This allows every<br>position in the decoder to attend over all positions in the input sequence. This mimics the<br>typical encoder-decoder attention mechanisms in sequence-to-sequence models such as<br>[38, 2, 9].<br>• The encoder contains self-attention layers. In a self-attention layer all of the keys, values<br>and queries come from the same place, in this case, the output of the previous layer in the<br>encoder. Each position in the encoder can attend to all positions in the previous layer of the<br>encoder.<br>• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to<br>all positions in the decoder up to and including that position. We need to prevent leftward')]\n",
      "[]\n",
      "[Document(metadata={'total_pages': 24}, page_content=\"<p id='153' data-category='paragraph' style='font-size:16px'>Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser,<br>Ł . Universal transformers. arXiv preprint arXiv:1807.03819 ,<br>2018.</p><br><p id='154' data-category='paragraph' style='font-size:16px'>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-<br>training of deep bidirectional transformers for language under-<br>standing. arXiv preprint arXiv:1810.04805 , 2018.</p><br><p id='155' data-category='paragraph' style='font-size:16px'>Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston,<br>J. Wizard of wikipedia: Knowledge-powered conversational<br>agents. arXiv preprint arXiv:1811.01241 , 2018.</p><br><p id='156' data-category='paragraph' style='font-size:16px'>Fan, A., Lewis, M., and Dauphin, Y. Hierarchical neural story<br>generation. arXiv preprint arXiv:1805.04833 , 2018.</p>\"), Document(metadata={'total_pages': 24}, page_content=\"<p id='76' data-category='paragraph' style='font-size:16px'>Greedy decoding from GPT-2 when conditioned on a doc-<br>ument, the history of the associated conversation, and a<br>ﬁnal token A: achieves 55 F1 on the development set. This<br>matches or exceeds the performance of 3 out of 4 base-<br>line systems without using the 127,000+ manually collected<br>question answer pairs those baselines were trained on. The<br>supervised SOTA, a BERT based system ( Devlin et al. ,</p>\"), Document(metadata={'total_pages': 24}, page_content=\"<br>supervised SOTA, a BERT based system ( Devlin et al. ,</p><br><table id='77' style='font-size:14px'><tr><td></td><td>R-1</td><td>R-2</td><td>R-L</td><td>R-AVG</td></tr><tr><td>Bottom-Up Sum</td><td>41.22</td><td>18.68</td><td>38.34</td><td>32.75</td></tr><tr><td>Lede-3</td><td>40.38</td><td>17.66</td><td>36.62</td><td>31.55</td></tr><tr><td>Seq2Seq + Attn</td><td>31.33</td><td>11.81</td><td>28.83</td><td>23.99</td></tr><tr><td>GPT-2 TL;DR:</td><td>29.34</td><td>8.27</td><td>26.58</td><td>21.40</td></tr><tr><td>Random-3</td><td>28.78</td><td>8.63</td><td>25.52</td><td>20.98</td></tr><tr><td>GPT-2 no hint</td><td>21.58</td><td>4.03</td><td>19.47</td><td>15.03</td></tr></table><br>\"), Document(metadata={'total_pages': 24}, page_content=\"<p id='15' data-category='paragraph' style='font-size:20px'>utilize a combination of pre-training and supervised ﬁne-<br>tuning. This approach has a long history with a trend to-<br>wards more ﬂexible forms of transfer. First, word vectors<br>were learned and used as inputs to task-speciﬁc architec-<br>tures ( Mikolov et al. , 2013 ) ( Collobert et al. , 2011 ), then<br>the contextual representations of recurrent networks were<br>transferred ( Dai & Le , 2015 ) ( Peters et al. , 2018 ), and re-<br>cent work suggests that task-speciﬁc architectures are no<br>longer necessary and transferring many self-attention blocks<br>is sufﬁcient ( Radford et al. , 2018 ) ( Devlin et al. , 2018 ).</p><br>\")]\n",
      "('Attention mechanism is a technique used in deep learning models to focus on '\n",
      " 'relevant parts of input data while ignoring the less important parts. It is '\n",
      " \"used in BERT and GPT to improve the model's ability to understand context \"\n",
      " 'and relationships between words in a sentence.\\n'\n",
      " '\\n'\n",
      " 'In BERT, the attention mechanism is used in the transformer layers to allow '\n",
      " 'each position in the input sequence to attend to all other positions. This '\n",
      " 'helps the model to capture long-range dependencies and better understand the '\n",
      " 'context of each word in the sentence.\\n'\n",
      " '\\n'\n",
      " 'In GPT, the attention mechanism is used to generate more accurate '\n",
      " 'predictions for the next word in a sequence. By attending to previous words '\n",
      " 'in the sequence, the model can better understand the context and generate '\n",
      " 'more relevant predictions.\\n'\n",
      " '\\n'\n",
      " 'Reference(s):\\n'\n",
      " '- Attention: https://en.wikipedia.org/wiki/Attention_(deep_learning)\\n'\n",
      " '- BERT: https://en.wikipedia.org/wiki/BERT_(language_model)\\n'\n",
      " '- GPT: https://en.wikipedia.org/wiki/Generative_pre-trained_transformer')\n"
     ]
    }
   ],
   "source": [
    "from utils import inference, generate_embeddings\n",
    "from pprint import pprint\n",
    "\n",
    "# generate_embeddings('./sample_pdfs/BERT-Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'bert')\n",
    "# generate_embeddings('./sample_pdfs/Language Models are Unsupervised Multitask Learners.pdf', 'gpt')\n",
    "\n",
    "output = inference(\"What is attention mechanism and how is it used in bert and gpt?\", ['attention', 'bert', 'gpt'])\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upstage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
